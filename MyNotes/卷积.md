## 卷积

1. 全连接

   全连接网络的参数个数： $\sum(前层\times 后层+后层)$

![conv0](.\pic\conv0.jpg)

<center> 
    <b>图5.1 全连接网络的参数量</b>
</center>


如图5-1所示，针对一张分辨率仅为28 * 28的黑白图像（像素值个数为28 * 28 * 1 = 784），全连接网络的参数总量就有将近40万个。

在实际应用中，图像的分辨率远高于此，且大多数是彩色图像。虽然全连接网络一般被认为是分类预测的最佳网络，但待优化的参数过多，容易导致模型过拟合。



2. 卷积神经网络

![conv1](.\pic\conv1.jpg)

<center> 
    <b>图5-4 神经网络中的卷积计算</b>
</center>



卷积的概念：卷积可以认为是一种有效提取图像特征的方法。一般会用一个正方形的卷积核，按指定步长，在输入特征图上滑动，遍历输入特征图中的每个像素点。每一个步长，卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项得到输出特征的一个像素点，如图5-4所示，利用大小为3×3×1的卷积核对5×5×1的单通道图像做卷积计算得到相应结果。



3. 对于彩色图像（多通道）来说，卷积核通道数与输入特征一致，套接后在对应位置上进行乘加和操作，如图5-5所示，利用三通道卷积核对三通道的彩色特征图做卷积计算。

   ![conv2](.\pic\conv2.jpg)

<center>
    <b>图5-5 三通道彩色图像的卷积计算</b>
</center>

> 用多个卷积核可实现对同一层输入特征的多次特征提取，卷积核的个数决定输出层的通道（channels）数，即输出特征图的深度。
>
> 感受野（Receptive Field）的概念：卷积神经网络各输出层每个像素点在原始图像上的映射区域大小，如图5-7所示。

![conv3](.\pic\conv3.jpg)

<center>
    <b>图5-6 感受野示意图</b>
</center>

当我们采用尺寸不同的卷积核时，最大的区别就是感受野的大小不同，所以经常会采用多层小卷积核来替换一层大卷积核，在保持感受野相同的情况下减少参数量和计算量，例如十分常见的用2层3 * 3卷积核来替换1层5 * 5卷积核的方法，如图5-7所示。

![conv4](.\pic\conv4.jpg)

<center>
    <b>图5-7 两层3*3卷积核与一层5*5卷积核的对比</b>
</center>

> 这里给出详细推导：不妨设输入特征图的宽、高均为x，卷积计算的步长为1，显然，两个3 * 3卷积核的参数量为9 + 9 = 18，小于5 * 5卷积核的25，前者的参数量更少。
> 在计算量上，根据图5-8所示的输出特征尺寸计算公式，对于5 * 5卷积核来说，输出特征图共有(x – 5 + 1)^2个像素点，每个像素点需要进行5 * 5 = 25次乘加运算，则总计算量为25 * (x – 5 + 1)^2 = 25x^2 – 200x + 400；
> 对于两个3 * 3卷积核来说，第一个3 * 3卷积核输出特征图共有(x – 3 + 1)^2个像素点，每个像素点需要进行3 * 3 = 9次乘加运算，第二个3 * 3卷积核输出特征图共有(x – 3 + 1 – 3 + 1)^2个像素点，每个像素点同样需要进行9次乘加运算，则总计算量为9 * (x – 3 + 1)^2 + 9 * (x – 3 + 1 – 3 + 1)^2 = 18 x^2 – 108x + 180；
> 对二者的总计算量（乘加运算的次数）进行对比，18 x^2 – 200x + 400 < 25x^2 – 200x + 400，经过简单数学运算可得x < 22/7 or x > 10，x作为特征图的边长，在大多数情况下显然会是一个大于10的值（非常简单的MNIST数据集的尺寸也达到了28 * 28），所以两层3 * 3卷积核的参数量和计算量，在通常情况下都优于一层5 * 5卷积核，尤其是当特征图尺寸比较大的情况下，两层3 * 3卷积核在计算量上的优势会更加明显。



输出特征尺寸计算：在了解神经网络中卷积计算的整个过程后，就可以对输出特征图的尺寸进行计算，如图5-8所示，5×5的图像经过3×3大小的卷积核做卷积计算后输出特征尺寸为3×3。

![conv5](.\pic\conv5.jpg)

<center>
    <b>图5-8 输出特征尺寸计算</b>
</center>

> 全零填充（padding）：为了保持输出图像尺寸与输入图像一致，经常会在输入图像周围进行全零填充，如图5-9所示，在5×5的输入图像周围填0，则输出特征尺寸同为5×5。

![conv6](.\pic\conv6.jpg)

<center>
    <b>图5-9 全零填充的输出特征尺寸计算</b>
</center>

在Tensorflow框架中，用参数padding = ‘SAME’或padding = ‘VALID’表示是否进行全零填充，其对输出特征尺寸大小的影响如下：
$$
padding = \left\{
\begin{aligned}
SAME\quad\qquad&\frac{入长}{步长}(面积不变)\\
VALID(不全零填充）&\frac{入长-核长+1}{步长}（向上取整）
\end{aligned}
\right.
$$
上下两行分别代表对输入图像进行全零填充或不进行填充，对于5×5×1的图像来说，当padding = ‘SAME’时，输出图像边长为5；当padding = ‘VALID’时，输出图像边长为3。

具备以上知识后，就可以在Tensorflow框架下利用Keras来构建CNN中的卷积层，使用的是tf.keras.layers.Conv2D函数，具体的使用方法如下：

```python
tf.keras.layers.Conv2D(
input_shape = (高, 宽, 通道数), #仅在第一层有
filters = 卷积核个数,
kernel_size = 卷积核尺寸,
strides = 卷积步长,
padding = ‘SAME’ or ‘VALID’,
activation = ‘relu’ or ‘sigmoid’ or ‘tanh’ or ‘softmax’等#如有BN则此处不用写
)
```

使用此函数构建卷积层时，需要给出的信息有：
A）输入图像的信息，即宽高和通道数；
B）卷积核的个数以及尺寸，如filters = 16, kernel_size = (3, 3)代表采用16个大小为3×3的卷积核；
C）卷积步长，即卷积核在输入图像上滑动的步长，纵向步长与横向步长通常是相同的，默认值为1；
D）是否进行全零填充，全零填充的具体作用上文有描述；
E）采用哪种激活函数，例如relu、softmax等，各种函数的具体效果在前面章节中有详细描
述；

这里需要注意的是，在利用Tensorflow框架构建卷积网络时，一般会利用BatchNormalization函数来构建BN层，进行批归一化操作，所以在Conv2D函数中经常不写BN。BN操作的具体含义和作用见下文。

> Batch Normalization(批标准化)：对一小批数据在网络各层的输出做标准化处理，其具体实现方式如图5-10所示。（标准化：使数据符合0均值，1为标准差的分布。）

![conv7](.\pic\conv7.jpg)

<center>
    <b>图5-10 Batch Normalization的实现</b>
</center>

Batch Normalization将神经网络每层的输入都调整到均值为0，方差为1的标准正态分布，其目的是解决神经网络中梯度消失的问题，如图5-11所示。

![conv8](.\pic\conv8.jpg)

<center>
    <b>图5-11 Batch Normalization的作用（以Sigmoid激活函数为例）</b>
</center>
BN操作将原本偏移的特征数据重新拉回到 0 均值，使进入激活函数的数据分布在激活函数线性区，使得输入数据的微小变化，更明显地体现到激活函数的输出，提升了激活函数对输入数据的区分力，但是这种简单的特征数据标准化，使特征数据完全满足标准正态分布，集中在激活函数中心的线性区域，使激活函数丧失了非线性特性，因此在BN操作中为每个卷积核引入了两个可训练参数**缩放因子γ以及偏移因子β** ，反向传播时，缩放因子γ以及偏移因子β 会与其他待训练参数一同被训练优化，使标准正态分布后的特征数据通过**缩放因子γ以及偏移因子β**优化了特征数据分布的宽窄和偏移量，保证了网络的非线性表达力。

BN操作的另一个重要步骤是缩放和偏移，值得注意的是，缩放因子γ以及偏移因子β都是可训练参数，其作用如图5-12所示。

![conv9](pic\conv9.jpg)

<center>
    <b>图5-12 BN中的缩放与平移</b>
</center>
BN操作通常位于卷积层之后，激活层之前，在Tensorflow框架中，通常使用Keras中
的tf.keras.layers.BatchNormalization函数来构建BN层。

在调用此函数时，需要注意的一个参数是training，此参数只在调用时指定，在模型进行前向推理时产生作用，当training = True时，BN操作采用当前batch的均值和标准差；当training = False时，BN操作采用滑动平均（running）的均值和标准差。在Tensorflow中，通常会指定training = False，可以更好地反映模型在测试集上的真实效果。

> 滑动平均（running）的解释：滑动平均，即通过一个个batch历史的叠加，最终趋向数据集整体分布的过程，在测试集上进行推理时，滑动平均的参数也就是最终保存的参数。

**池化（pooling）：池化的作用是减少特征数量（降维）。最大值池化可提取图片纹理，均值池化可保留背景特征**，如图5-13所示。

![conv10](pic\conv10.jpg)

<center>
    <b>图5-13 最大值池化与均值池化</b>
</center>

在Tensorflow框架下，可以利用Keras来构建池化层，使用的是tf.keras.layers.MaxPool2D函数和tf.keras.layers.AveragePooling2D函数，具体的使用方法如下：

``` python
tf.keras.layers.MaxPool2D(
pool_size = 池化核尺寸,
strides = 池化步长,
padding = ‘SAME’ or ‘VALID’
)
tf.keras.layers.AveragePooling2D(
pool_size = 池化核尺寸,
strides = 池化步长,
padding = ‘SAME’ or ‘VALID’
)
```

舍弃（Dropout）：在神经网络的训练过程中，将一部分神经元按照一定概率从神经网络中暂时舍弃，使用时被舍弃的神经元恢复链接，如图5-14所示。

![conv11](pic\conv11.jpg)

<center>
    <b>图5-14 舍弃（Dropout）示意图</b>
</center>

在Tensorflow框架下，利用tf.keras.layers.Dropout函数构建Dropout层，参数为舍弃的概率（大于0小于1）。
利用上述知识，就可以构建出基本的卷积神经网络（CNN）了，其核心思路为在CNN中利用卷积核（kernel）提取特征后，送入全连接网络。

> CNN模型的主要模块：一般包括上述的卷积层、BN层、激活函数、池化层以及全连接层，如图5-15所示。

![conv12](pic\conv12.jpg)

<center>
    <b>图5-15 CNN模型的主要模块</b>
</center>



在此基础上，可以总结出在Tensorflow框架下，利用Keras来搭建神经网络的“八股”套路，在主干的基础上，还可以添加其他内容，来完善神经网络的功能，如利用自己的图片和标签文件来自制数据集；通过旋转、缩放、平移等操作对数据集进行数据增强；保存模型文件进行断点续训；提取训练后得到的模型参数以及准确率曲线，实现可视化等。



>构建神经网络的“八股”套路：
>A）import引入tensorflow及keras、numpy等所需模块。
>B）读取数据集，课程中所利用的MNIST、cifar10等数据集比较基础，可以直接从sklearn等模块中引入，但是在实际应用中，大多需要从图片和标签文件中读取所需的数据集。
>C）搭建所需的网络结构，当网络结构比较简单时，可以利用keras模块中的tf.keras.Sequential来搭建顺序网络模型；但是当网络不再是简单的顺序结构，而是有其它特殊结构出现时（例如ResNet中的跳连结构），便需要利用class来定义自己的网络结构。前者使用起来更加方便，但实际应用中往往需要利用后者来搭建网络。
>D）对搭建好的网络进行编译（compile），通常在这一步指定所采用的优化器（如Adam、sgd、RMSdrop等）以及损失函数（如交叉熵函数、均方差函数等），选择哪种优化器和损
>失函数往往对训练的速度和效果有很大的影响，至于具体如何进行选择，前面的章节中有比较详细的介绍。
>E）将数据输入编译好的网络来进行训练（model.fit），在这一步中指定训练轮数epochs以及batch_size等信息，由于神经网络的参数量和计算量一般都比较大，训练所需的时间也会比较长，尤其是在硬件条件受限的情况下，所以在这一步中通常会加入断点续训以及模型参数保存等功能，使训练更加方便，同时防止程序意外停止导致数据丢失的情况发生。
>F）将神经网络模型的具体信息打印出来（model.summary），包括网络结构、网络各层的参数等，便于对网络进行浏览和检查。