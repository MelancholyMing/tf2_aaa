## 集成学习（ensemble learning）

_也叫 **多分类器系统（multi-classifier system）、基于委员会的学习（committee-based learning）**_



> 通过多个学习器完成任务
>
> + 个体学习器（individual learner）：
>   1. 同种算法的集成
>   2. 同质集成（homogeneous）
>   3. 个体学习器，基学习器（base learner）
>   4. 基学习算法（base learning algorithm）
>
> + 组件学习器（component learner）：
>   1. 异质的（heterogeneous）
>
> 原则：
>
> ​	**好而不同**
>
> 分类：
>
> * 个体学习器之间存在强依赖关系，必须串行生成的序列化方法（**Boosting**）
> * 个体学习器之间不存在强依赖关系，可同时生成的并行化方法（**Bagging，随机森林**）





**Boosting**：

+ AdaBoosting
+ Gradient_boosting(GBDT):
  + 原理：
    + GBDT为什么这么优秀？一是效果确实挺不错。二是既可以用于分类也可以用于回归。三是可以筛选特征。
    + 什么是GBDT？ GBDT是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。
    + GBDT的训练过程？ 训练一个模型m1，产生错误e1，针对e1训练第二个模型m2，产生错误e2，针对错误e2训练第三个模型m3，产生错误e3...，最终的预测结果是m1+m2+m3+.....。
    + 在sklearn中梯度提升回归树有四种可选的损失函数，分别为`'ls:平方损失'，'lad:绝对损失','huber:huber损失'，'quantile:分位数损失'`，而在sklearn中梯度提升分类树有两种可选的损失函数，一种是`'exponential:指纹损失'`，一种是`'deviance:对数损失'`。

**Bagging与随机森林**：

+ 目的：保证 “好而不同”的不同

+ 思路：从训练集进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。

+ 分类：取样方法：

  + Bagging：放回取样

  + Pasting：不放回取样

  + bootstrap_features:在特征空间随机采样

  + 即针对样本，又针对特征进行随机采样

  + 8.3.1 Bagging

  + 8.3.2 随机森林

    + 思路：样本和节点判断的特征都随机选，既假样本扰动又添加属性扰动

    + 对比Bagging与随机森林：当分类器数量比较少时，随机森林表现不好

      ​											 当分类器数量增加，随机森林的泛化性能优于Bagging

    + 补充：Extra-Trees：特征随机，样本不随机

      ​	ET 与 RF 的区别：1. RF应用了Bagging进行随机抽样，而ET的每棵决策树应用的是相同的样本

      ​								   2. RF在一个随机子集内基于信息熵和基尼指数寻找最优属性，而ET完全随机寻找一个特征值进行划分



##### 结合策略

+ 平均法

+ 加权平均法

  `性能比较：加权平均法的权重一般是从训练数据中学习而得，现实任务中的训练样本通常存在不充分或存在噪声，这将使得学出的权重不完全可靠，尤其是对规模比较大的集成来说，要学习的权重比较多，较容易致过拟合。因此，实验和应用均显示出，加权平均法未必一定优于简单平均法。一般而言，在个体学习器性能相近时宜使用简单平均法`

+ 投票法

  + 绝对多数投票法

  + 相对多数投票法

  + 加权投票法

  + 软投票与硬投票

    eg:

    **Hard Voting:**

    | 模型1 | A-99% | B-1%  |
    | :---: | :---: | :---: |
    | 模型2 | A-49% | B-51% |
    | 模型3 | A-40% | B-60% |
    | 模型4 | A-90% | B-10% |
    | 模型5 | A-30% | B-70% |

    **A-两票；B-三票，最终结果为B**

    **缺点：在很多情况下，少数服从多数并不是最合理的，所以考虑加入权值，由此引入soft集成**

    **Soft Voting：**

    **A-（0.99+0.49+0.4+0.9+0.3）/5 = 0.616**

    **B-（0.01+0.51+0.6+0.1+0.7）/5 = 0.384	最终结果为A**

    要求集合的每一个模型都能求估计概率

  ```
  能够估计概率的模型：
  1. KNN, K个近邻中数量最多的那个类的数量除以 K 就是概率
  2. 决策树，叶子节点中哪个类数量最大的就是那个类，概率就是数量最大的那个类除以叶子结点的类
  3. SVC，SVC本身是没有考虑概率的，它是寻找一个margin的最大值。但是也是可以计算的，不过需要消耗大量的计算资源
  ```

+ 学习法：stacking

  当训练数据很多时，一种更为强大的结合策略是使用 “学习法”，即通过另一个学习器来进行结合。Stacking 时学习法的典型代表，这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。

  思路：Stacking的策略是把训练样本集分为两部分，一部分用来训练初级学习器。通过初级分类器预测的结果和另外一部分样本一起用来训练次级学习器，由此得到最终的预测结果。


  过拟合问题：每个模型本身就有很多的超参数，需要有几层，又加上每一层使用的学习器的数量又是一个超参数，因此stacking就会复杂很多，也正是因为这种复杂性导致其容易产生过拟合