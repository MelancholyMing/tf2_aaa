## 21/1/31

#### 激活函数

建议：

+ 首选relu激活函数
+ 学习率设置较小值
+ 输入特征标准化，即让输入特征满足以 0 为均值，1 为标准差的正态分布
+ 初始化参数中心化，既让随机生成的参数满足以 0 为均值，$\sqrt{\frac{2}{当前层输入特征个数}}$  为标准差的正态分布。



#### 过拟合与欠拟合

1. 欠拟合的解决方法
   + 增加输入特征项
   + 增加网络参数
   + 减少正则化参数
2. 过拟合的解决方法
   + 数据清洗
   + 增大训练集
   + 采用正则化
   + 增大正则化参数

#### 神经网络参数优化器

待优化参数 **w**，损失函数 **loss**，学习率 **lr**，每次迭代一个 **batch**，**t** 表示当前 **batch** 迭代的总次数：

1. 计算 **t** 时刻损失函数关于当前参数的梯度 $g_t = \nabla{loss} = \frac{\partial{loss}}{\partial(w_t)}$
2. 计算 **t** 时刻一阶动量 $m_t$ 和二阶动量 $V_t$
3. 计算 **t** 时刻下降梯度：$\eta_t = lr\cdot{m_t}/\sqrt{V_t}$
4. 计算 **t+1** 时刻参数：$w_{t+1} = w_t -\eta_t = w_t-lr\cdot{m_t/\sqrt{V_t}}$



+ 一阶动量：与梯度相关的函数
+ 二阶动量：与梯度平方相关的函数

###### 优化器

+ SGD（无momentum）

  $m_t = g_t\qquad V_t=1$

  $\eta_t = lr\cdot{m_t/\sqrt{V_t}} = lr\cdot{g_t}$

  $\begin{align}w_{t+1} &= w_t - \eta_t\\&=w_t-lr\cdot m_t/\sqrt{V_t}\\&=w_t-lr\cdot{g_t}\\&=w_t-lr*\frac{\partial{loss}}{\partial{w_t}}\end{align}$

+ SGDM(含momentum的SGD)，在SGD基础上增加一阶动量

  $m_t = \beta\cdot{m_{t-1}}+(1-\beta)\cdot{g_t}\qquad V_t = 1$

  $\begin{align}\eta_t &= lr\cdot{m_t/\sqrt{V_t}}\\ &= lr\cdot{m_t}\\&=lr\cdot(\beta\cdot{m_{t-1}}+(1-\beta)\cdot{g_t})\end{align}$

  $w_{t+1} = w_t - \eta_t=w_t-lr\cdot(\beta\cdot{m_{t-1}}+(1-\beta)\cdot{g_t})$

+ Adagrad，在SGD基础上增加二阶动量

  $m_t = g_t\qquad V_t=\sum\limits_{\tau=1}^tg_{\tau}^2$

  $\begin{align}\eta_t &= lr\cdot{m_t/\sqrt{V_t}}\\ &= lr\cdot{g_t/(\sum\limits_{\tau=1}^tg_{\tau}^2)}\end{align}$

  $\begin{align}w_{t+1} &= w_t - \eta_t\\&= w_t-lr\cdot{g_t/(\sum\limits_{\tau=1}^tg_{\tau}^2)}\end{align}$

+ RMSProp，SGD基础上增加二阶动量

  $m_t = g_t\qquad V_t=\beta\cdot{V_{t-1}}+(1-\beta)\cdot{g_t^2}$

  $\begin{align}\eta_t &= lr\cdot{m_t/\sqrt{V_t}}\\ &= lr\cdot{g_t/(\sqrt{\beta\cdot{V_{t-1}}+(1-\beta)\cdot{g_t^2}})}\end{align}$

  $\begin{align}w_{t+1} &= w_t - \eta_t\\&= w_t-lr\cdot{g_t/(\sqrt{\beta\cdot{V_{t-1}}+(1-\beta)\cdot{g_t^2}})}\end{align}$

+ Adam，同时结合SGDM一阶动量和RMSProp二阶动量

  $m_t = \beta_1\cdot{m_{t-1}}+(1-\beta_1)\cdot{g_t}$

  修正一阶动量的偏差：$\widehat{m_t} = \frac{m_t}{1-\beta_1^t}$

  $V_t=\beta_2\cdot{V_{t-1}}+(1-\beta_2)\cdot{g_t^2}$

  修正二阶动量的偏差：$\widehat{V_t} = \frac{V_t}{1-\beta_2^t}$

  $\begin{align}
  \eta_t &= lr\cdot\widehat{m_t}/\sqrt{\widehat{V_t}}\\
  	   &= lr\cdot\frac{m_t}{1-\beta_1^t}/\sqrt{\frac{V_t}{1-\beta_2^t}}
  \end{align}$

  $\begin{align}w_{t+1} &= w_t - \eta_t\\
  					   &= w_t-lr\cdot\frac{m_t}{1-\beta_1^t}/\sqrt{\frac{V_t}{1-\beta_2^t}}
  \end{align}$

  