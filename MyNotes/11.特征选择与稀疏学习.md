## 11.特征选择与稀疏学习



### 11.1 子集搜索与评价

1. 子集搜索与评价的目的

   从特征集合中选取一个包含了所有重要信息特征的子集

   

2. 子集搜索

   - **前向搜索（forward）**

     **_单特征子集_**：给定特征集合 $\{a_1,a_2,\cdots,a_d\}$ ，我们可将每个特征看作一个候选子集，对这 $d$ 个候选单特征子集进行评价，假定 $\{a_2\}$ 最优，于是将 $\{a_2\}$ 作为第一轮的选定集；

     **_两个特征子集_**：然后，在上一轮的选定集中假如一个特征，构成包含两个特征的候选子集，假定在这 $d-1$ 个候选两特征子集中 $\{a_2,a_4\}$ 最优，且优于 $\{a_2\}$ ，于是将 $\{a_2,a_4\}$ 作为本轮的选定集；  

     **_停止_**： 假定在第 $k+1$ 轮时，最优的候选 $(k+1)$ 特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的 $k$ 特征集合作为特征选择结果。这样逐渐增加相关特征的策略被称为 “前向” 搜索。

   - **后向搜索（backward）**

     类似的，若我们从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略称为 “后向” 搜索。

   - **双向搜索（bidirectional）**

     还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征（这些特征在后续轮中将确定不会被去除）、同时减少无关特征，这样的策略称为 “双向” 搜索

   - **问题**

     显然，上述策略都是贪心的，因为他们仅考虑了使本轮选定集最优。例如在第三轮假定选择 $a_5$ 优于 $a_6$ ，于是选定集为 $\{a_2,a_4,a_5\}$ ,然而在第四轮却可能是 $\{a_2,a_4,a_6,a_8\}$ 比所有的 $\{a_2,a_4,a_5,a_i\}$ 都更优。遗憾的是，若不进行穷举搜索，则这样的问题无法避免。

3. 子集评价

   给定数据集D，假定 D 中第 $i$ 类样本所占的比例 $p_i(i=1,2,\dots,|\mathcal{Y}|)$ .为便于讨论，假定样本属性均为离散型，对属性子集A，假定根据其取值将 D 分成了 V 个子集 $\{D^1,D^2,\dots,D^V\}$，每个子集中的样本在 A 上取值相同，于是我们可计算属性子集 A 的信息增益 $Gain(A) = Ent(D) - \sum\limits_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)\tag{11.1}$，

   

   + 信息熵

     其中信息熵定义为 $Ent(D) = -\sum\limits_{i=1}^{|\mathcal{Y}|}p_k\log_2^{p_k},\tag{11.2}$ 

      信息增益越大，意味着特征子集A包含的有助于分类的信息越多，于是，对每个候选特征子集，我们可基于训练数据集 D 来计算其信息增益，以此作为评价准则。

   + 其他方法

     更一般地，特征子集 A 实际上确定了对数据集 D 的一个划分，每个划分区域对应着 A 上的一个取值。而样本标记信息 Y 则对应着对 D 的真是划分，通过估算这两个划分的差异，就能对 A 进行评价，与 Y 对应的划分的差异越小，则说明 A 越好。信息熵仅是判断这个差异的一种途径，其他能判断这两个划分差异的机制都能用于特征子集评价。

4. 特征选择

   将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。例如将前向搜索与信息熵相结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树结点的划分属性所组成的集合就是选择出的特征子集。其他特征的选择方法未必像决策树特征选择这么明显，但它们在本质上都是显示或隐式地结合了某种（或多种）子集搜索机制和子集评价机制。

5. 常见特征选择方法

   + 过滤式filter
   + 包裹式wrapper
   + 嵌入式embedding

