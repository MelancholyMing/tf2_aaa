{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED  =23455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = np.random.RandomState(seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rdm.rand(32,2)\n",
    "y_ = [[x1+x2 + (rdm.rand()/10-0.05)] for (x1,x2) in x]  # 生成噪声[0,1)/10 = [0,0.1);[0,0.1)-0.05=[-0.05,0.05)\n",
    "x = tf.cast(x,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps, w1 is \n",
      "[[-0.8096241]\n",
      " [ 1.4855157]] \n",
      "\n",
      "after 500 training steps, w1 is \n",
      "[[-0.21934733]\n",
      " [ 1.6984866 ]] \n",
      "\n",
      "after 1000 training steps, w1 is \n",
      "[[0.0893971]\n",
      " [1.673225 ]] \n",
      "\n",
      "after 1500 training steps, w1 is \n",
      "[[0.28368822]\n",
      " [1.5853055 ]] \n",
      "\n",
      "after 2000 training steps, w1 is \n",
      "[[0.423243 ]\n",
      " [1.4906037]] \n",
      "\n",
      "after 2500 training steps, w1 is \n",
      "[[0.531055 ]\n",
      " [1.4053345]] \n",
      "\n",
      "after 3000 training steps, w1 is \n",
      "[[0.61725086]\n",
      " [1.332841  ]] \n",
      "\n",
      "after 3500 training steps, w1 is \n",
      "[[0.687201 ]\n",
      " [1.2725208]] \n",
      "\n",
      "after 4000 training steps, w1 is \n",
      "[[0.7443262]\n",
      " [1.2227542]] \n",
      "\n",
      "after 4500 training steps, w1 is \n",
      "[[0.7910986]\n",
      " [1.1818361]] \n",
      "\n",
      "after 5000 training steps, w1 is \n",
      "[[0.82943517]\n",
      " [1.1482395 ]] \n",
      "\n",
      "after 5500 training steps, w1 is \n",
      "[[0.860872 ]\n",
      " [1.1206709]] \n",
      "\n",
      "after 6000 training steps, w1 is \n",
      "[[0.88665503]\n",
      " [1.098054  ]] \n",
      "\n",
      "after 6500 training steps, w1 is \n",
      "[[0.90780276]\n",
      " [1.0795006 ]] \n",
      "\n",
      "after 7000 training steps, w1 is \n",
      "[[0.92514884]\n",
      " [1.0642821 ]] \n",
      "\n",
      "after 7500 training steps, w1 is \n",
      "[[0.93937725]\n",
      " [1.0517985 ]] \n",
      "\n",
      "after 8000 training steps, w1 is \n",
      "[[0.951048]\n",
      " [1.041559]] \n",
      "\n",
      "after 8500 training steps, w1 is \n",
      "[[0.96062106]\n",
      " [1.0331597 ]] \n",
      "\n",
      "after 9000 training steps, w1 is \n",
      "[[0.9684733]\n",
      " [1.0262702]] \n",
      "\n",
      "after 9500 training steps, w1 is \n",
      "[[0.97491425]\n",
      " [1.0206193 ]] \n",
      "\n",
      "after 10000 training steps, w1 is \n",
      "[[0.9801975]\n",
      " [1.0159837]] \n",
      "\n",
      "after 10500 training steps, w1 is \n",
      "[[0.9845312]\n",
      " [1.0121814]] \n",
      "\n",
      "after 11000 training steps, w1 is \n",
      "[[0.9880858]\n",
      " [1.0090628]] \n",
      "\n",
      "after 11500 training steps, w1 is \n",
      "[[0.99100184]\n",
      " [1.0065047 ]] \n",
      "\n",
      "after 12000 training steps, w1 is \n",
      "[[0.9933934]\n",
      " [1.0044063]] \n",
      "\n",
      "after 12500 training steps, w1 is \n",
      "[[0.9953551]\n",
      " [1.0026854]] \n",
      "\n",
      "after 13000 training steps, w1 is \n",
      "[[0.99696386]\n",
      " [1.0012728 ]] \n",
      "\n",
      "after 13500 training steps, w1 is \n",
      "[[0.9982835]\n",
      " [1.0001147]] \n",
      "\n",
      "after 14000 training steps, w1 is \n",
      "[[0.9993659]\n",
      " [0.999166 ]] \n",
      "\n",
      "after 14500 training steps, w1 is \n",
      "[[1.0002553 ]\n",
      " [0.99838644]] \n",
      "\n",
      "final w1 is : [[1.0009792]\n",
      " [0.9977485]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random.normal([2,1],stddev=1,seed=1))\n",
    "epoch = 15000\n",
    "lr = 0.002\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.matmul(x,w1)\n",
    "        loss_mse = tf.reduce_mean(tf.square(y-y_))\n",
    "    grads = tape.gradient(loss_mse,w1)\n",
    "    w1.assign_sub(lr*grads)\n",
    "    \n",
    "    if epoch%500 == 0:\n",
    "        print(\"after %d training steps, w1 is \"%(epoch))\n",
    "        print(w1.numpy(),\"\\n\")\n",
    "        \n",
    "print(\"final w1 is :\",w1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 training steps, w1 is \n",
      "[[1.0350155]\n",
      " [0.9322055]] \n",
      "\n",
      "after 500 training steps, w1 is \n",
      "[[0.93260807]\n",
      " [0.94418126]] \n",
      "\n",
      "after 1000 training steps, w1 is \n",
      "[[0.9360667 ]\n",
      " [0.94793314]] \n",
      "\n",
      "after 1500 training steps, w1 is \n",
      "[[0.9354001]\n",
      " [0.9468998]] \n",
      "\n",
      "after 2000 training steps, w1 is \n",
      "[[0.93473357]\n",
      " [0.9458664 ]] \n",
      "\n",
      "after 2500 training steps, w1 is \n",
      "[[0.934067  ]\n",
      " [0.94483304]] \n",
      "\n",
      "after 3000 training steps, w1 is \n",
      "[[0.93340045]\n",
      " [0.9437997 ]] \n",
      "\n",
      "after 3500 training steps, w1 is \n",
      "[[0.9368591 ]\n",
      " [0.94755155]] \n",
      "\n",
      "after 4000 training steps, w1 is \n",
      "[[0.9361925]\n",
      " [0.9465182]] \n",
      "\n",
      "after 4500 training steps, w1 is \n",
      "[[0.93552595]\n",
      " [0.9454848 ]] \n",
      "\n",
      "after 5000 training steps, w1 is \n",
      "[[0.9352799 ]\n",
      " [0.94806296]] \n",
      "\n",
      "after 5500 training steps, w1 is \n",
      "[[0.93461335]\n",
      " [0.9470296 ]] \n",
      "\n",
      "after 6000 training steps, w1 is \n",
      "[[0.9339468]\n",
      " [0.9459962]] \n",
      "\n",
      "after 6500 training steps, w1 is \n",
      "[[0.9369849]\n",
      " [0.9461366]] \n",
      "\n",
      "after 7000 training steps, w1 is \n",
      "[[0.9326137]\n",
      " [0.9439295]] \n",
      "\n",
      "after 7500 training steps, w1 is \n",
      "[[0.9360723 ]\n",
      " [0.94768137]] \n",
      "\n",
      "after 8000 training steps, w1 is \n",
      "[[0.93540573]\n",
      " [0.946648  ]] \n",
      "\n",
      "after 8500 training steps, w1 is \n",
      "[[0.9347392 ]\n",
      " [0.94561464]] \n",
      "\n",
      "after 9000 training steps, w1 is \n",
      "[[0.9340726]\n",
      " [0.9445813]] \n",
      "\n",
      "after 9500 training steps, w1 is \n",
      "[[0.93340605]\n",
      " [0.9435479 ]] \n",
      "\n",
      "final w1 is : [[0.9359125]\n",
      " [0.9462832]]\n"
     ]
    }
   ],
   "source": [
    "COST = 99\n",
    "PROFIT = 1\n",
    "epoch = 10000\n",
    "for epoch in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.matmul(x,w1)\n",
    "        loss_mse = tf.reduce_mean(tf.where(tf.greater(y,y_),(y-y_)*COST,(y_-y)*PROFIT))\n",
    "    grads = tape.gradient(loss_mse,w1)\n",
    "    w1.assign_sub(lr*grads)\n",
    "    \n",
    "    if epoch%500 == 0:\n",
    "        print(\"after %d training steps, w1 is \"%(epoch))\n",
    "        print(w1.numpy(),\"\\n\")\n",
    "        \n",
    "print(\"final w1 is :\",w1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce1 = tf.losses.categorical_crossentropy([1,0],[0.6,0.4])\n",
    "loss_ce2 = tf.losses.categorical_crossentropy([1,0],[0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5108256, shape=(), dtype=float32)\n",
      "tf.Tensor(0.22314353, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss_ce1)\n",
    "print(loss_ce2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    h1 = tf.matmul(x_train, w1) + b1\n",
    "    h1 = tf.nn.relu(h1)\n",
    "    y = tf.matmul(h1,w2) + b2\n",
    "    \n",
    "    loss_mse = tf.reduce_mean(tf.square(y_train-y))\n",
    "    \n",
    "    loss_regularization = []\n",
    "    loss_regularization.append(tf.nn.l2_loss(w1))\n",
    "    loss_regularization.append(tf.nn.l2_loss(w2))\n",
    "    loss_regularization = tf.reduce_sum(loss_regularization)\n",
    "    loss = loss_mse + 0.3*loss_regularization\n",
    "    \n",
    "variables = [w1,b1,w2,b2]\n",
    "grads = tape.gradient(loss,variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
