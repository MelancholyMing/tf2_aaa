# 循环神经网络



### 1. 循环神经网络（Recurrent Neural Network，RNN）



1. 卷积神经网络与循环神经网络简单对比

   CNN: 借助卷积核（kernel）提取特征后，送入后续网络(如全连接网络Dense)进行分类、目标检测等操作。CNN借助卷积核从空间维度提取信息，卷积核参数空间共享。
   RNN: 借助循环核（cell）提取特征后，送入后续网络(如全连接网络Dense)进行预测等操作。RNN借助循环核从时间维度提取信息，循环核参数时间共享。

2. RNN

   + 循环核

     ![循环核](pic\rnn_kernel.jpg)

     

     <center>
         <b>图1.2.1 RNN循环核</b>
     </center>

     循环核具有记忆力，通过不同时刻的参数共享，实现了对时间序列的信息提取。每个循环核有多个记忆体，对应图1.2.1中的多个小圆柱。记忆体内存储着每个时刻的状态信息 $h_t$，这里 $h_t = \tanh(x_tw_{xh} + h_{t-1}w_{hh} + bh)$。其中，$w_{xh}$、$w_{hh}$为权重矩阵，$bh$ 为偏置，$x_t$ 为当前时刻的输入特征，$h_{t-1}$ 为记忆体上一时刻存储的状态信息，tanh 为激活函数。

     当前时刻循环核的输出特征 $y_t = softmax(h_tw_{hy} + by)$，其中$w_{hy}$为权重矩阵、$by$ 为偏置、$softmax$ 为激活函数，其实就相当于一层全连接层。我们可以设定记忆体的个数从而改变记忆容量，当记忆体个数被指定、输入$x_t$输出$y_t$维度被指定，周围这些待训练参数的维度也就被限定了。在前向传播时，记忆体内存储的状态信息 $h_t$ 在每个时刻都被刷新，而三个参数矩阵$w_{xh}$、$w_{hh}$、$w_{hy}$和两个偏置项$bh$、$by$ 自始至终都是固定不变的。在反向传播时，三个参数矩阵和两个偏置项由梯度下降法更新。

     

   + 循环核按时间步展开

     将循环核按时间步展开，就是把循环核按照时间轴方向展开，可以得到如图1.2.2的形式。每个时刻记忆体状态信息 $h_t$ 被刷新，记忆体周围的参数矩阵和两个偏置项是固定不变的，我们训练优化的就是这些参数矩阵。训练完成后，使用效果最好的参数矩阵执行前向传播，然后输出预测结果。其实这和我们人类的预测是一致的：我们脑中的记忆体每个时刻都根据当前的输入而更新；当前的预测推理是根据我们以往的知识积累用固化下来的“参数矩阵”进行的推理判断。
     可以看出，循环神经网络就是借助循环核实现时间特征提取后把提取到的信息送入全连接网络，从而实现连续数据的预测。

     ![](pic\rnn_1.jpg)

     <center>
         <b>图1.2.2 RNN循环核按时间步展开</b>
     </center>

     

   + 循环计算层：向输出方向生长

     在RNN中，每个循环核构成一层循环计算层，循环计算层的层数是向输出方向增长的。如图1.2.3所示，左图的网络有一个循环核，构成了一层循环计算层；中图的网络有两个循环核，构成了两层循环计算层；右图的网络有三个循环核，构成了三层循环计算层。其中，三个网络中每个循环核中记忆体的个数可以根据我们的需求任意指定。

     ![](pic\rnn_2.jpg)

     <center>
         <b>图1.2.3 循环计算层</b>
     </center>

     

   + RNN训练

     得到RNN的前向传播结果之后，和其他神经网络类似，我们会定义损失函数，使用反向传播梯度下降算法训练模型。RNN唯一的区别在于：由于它每个时刻的节点都可能有一个输出，所以RNN的总损失为所有时刻（或部分时刻）上的损失和。

   + Tensorflow2描述循环计算层

     tf.keras.layers.SimpleRNN(神经元个数，activation=‘激活函数’，return_sequences=是否每个时刻输出$h_t$到下一层)

     (1)神经元个数：即循环核中记忆体的个数
     (2) return_sequences：在输出序列中，返回最后时间步的输出值ℎ𝑡𝑡还是返回全部时间步的输出。False返回最后时刻(图1.2.5)，True返回全部时刻(图1.2.4)。当下一层依然是RNN层，通常为True，反之如果后面是Dense层，通常为Fasle。

     ![](pic\rnn_3.jpg)

     <center>
         <b>图1.2.4 return_sequences = True</b>
     </center>

     ![](pic\rnn_4.jpg)

     <center>
         <b>图1.2.5 return_sequences = False</b>
     </center>

     (3)输入维度：三维张量(输入样本数, 循环核时间展开步数， 每个时间步输入特征个数)。

     ![](pic\rnn_5.jpg)

     <center>
         <b>图1.2.6 RNN层输入维度</b>
     </center>

     如图1.2.6所示，左图一共要送入RNN层两组数据，每组数据经过一个时间步就会得到输出结果，每个时间步送入三个数值，则输入循环层的数据维度就是[2, 1, 3]；右图输入只有一组数据，分四个时间步送入循环层，每个时间步送入两个数值 ，则输入循环层的数据维度就是 [1，4， 2]。

     (4)输出维度：当return_sequenc=True，三维张量(输入样本数, 循环核时间展开步数,本层的神经元个数)；当return_sequenc=False，二维张量(输入样本数,本层的神经元个数)

     (5) activation：‘激活函数’(不写默认使用tanh）
     例：SimpleRNN(3, return_sequences=True)，定义了一个具有三个记忆体的循环核，这个循环核会在每个时间步输出 $h_t$。

   + LSTM、GRU

     RNN面临的较大问题是无法解决长跨度依赖问题，即后面节点相对于跨度很大的前面时间节点的信息感知能力太弱。如图2.1.1中的两句话：左上角的句子中sky可以由较短跨度的词预测出来，而右下角句子中的French与较长跨度之前的France有关系，即长跨度依赖，比较难预测。

     

     

     ![rnn_6](file://D:/workspace/MyNotes/pic/rnn_6.jpg?lastModify=1613962748)

     长跨度依赖的根本问题在于，多阶段的反向传播后会导致梯度消失、梯度爆炸。可以使用梯度截断去解决梯度爆炸问题，但无法轻易解决梯度消失问题。

     [举一个例子来解释RNN梯度消失和爆炸的原因](https://zhuanlan.zhihu.com/p/28687529)

     <center>
         <b>图2.1.1 RNN长跨度依赖问题的影响</b>
     </center>

     

     [LSTM、GRU](https://www.jianshu.com/p/9dc9f41f0b29)

   + 