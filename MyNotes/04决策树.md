### 04决策树 

+ 4.1 基本流程  

+ 4.2划分选择： 

  1. 信息增益（决策树D3训练算法） 

     实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法不直接使用信息增益，而是使用“增益率”来选择最优划分属性。采用与式(4.2)相同的符号表示，增益率定义为：

     ​			 			$Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}\tag{4.3}$

      其中，$$IV(a) = -\sum\limits_{v=1}^{D}\frac{|D^v|}{|D|}\log_2^{\frac{|D^v|}{|D|}}\tag{4.4}$$，称为属性 $a$ 的“固有值”，属性 $a$ 的可能取值数目越多（即 $V$ 越大），则 $IV(a)$ 的值通常会越大，例如，对表 4.1 的西瓜数据集2.0，有 $IV(触感) = 0.874 (V = 2)，IV(色泽) = 1.580(V = 3)，IV(编号)=4.088(V=17)$

  2. 增益率 （决策树C4.5训练算法）

  3. 基尼指数 （决策树CART训练算法） 

     - 分类树：基尼指数最小准则 
     - 回归树：平方误差最小准则

     > 基尼指数的概念： 
     >
     > 定义:  Gini指数越小表示集合中被选中的样本成分被分错的概率越小,也就是集合中的纯度越高。即**基尼指数**（<font color = green>基尼不纯度</font>）= 样本被选中的概率 $\times$ 样本被分错的概率 
     >
     > 公式：
     > $$
     > Gini(P) = \sum_{k=1}^{K}p_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2
     > $$
     > 
     >
     > 
     >
     > 说明：
     >
     > 1. $p_k$ 表示选中的样本属于 $k$ 类别的概率，则这个样本被分错的概率是 $1 - p_k$
     >
     > 2.  样本集合中有 $K$ 个类别，一个随机选中的样本可以属于这 $K$ 个类别中的任意一个，因而对类别做加和。
     > 3. 当为二分类时，$Gini(P) = 2p(1-p)$ .  
     >
     > CART决策树使用“基尼指数”来选择划分属性，采用与式（4.1）相同的符号，数据集 $D$ 的纯度可用基尼值来度量：
     >
     >  $$\begin{align} Gini(D) &= \sum_{k=1}^{|\mathcal{Y}|}{\sum_{k\prime\not= k}p_kp_{k\prime}}\notag\\ &=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2\tag{4.2} \end{align}$$ 
     >
     > 直观来说，$Gini(D)$ 反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，因此，$Gini(D)$ 越小，则数据集 $D$ 的纯度越高。采用与式（4.2）相同的符号表示，属性 $a$ 的基尼指数定义为：$$Gini\_index(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v).\tag{4.6}$$ 

     

+ 4.3剪枝处理： 
  
  1. 预剪枝 
  
     _自上而下_
  
  2. 后剪枝  
  
     _自下而上_
  
+ 4.4连续值与缺失值 
  
  1. 连续值处理 
   + 二分法，使得信息增益最大的阈值
  2. 缺失值处理  
     + C4.5例子
     + 其他方法：
       + 离散值：众数填充、相关性最高的列填充
       + 连续值：中位数、相关性最高的列做线性回归进行估计
  
+ 4.5多变量决策树 



> YJango笔记： 
>
> + 信息熵  
>
>   1. 熵： 一种事物的不确定性叫做熵（eg：买榴莲，该挑哪个，不确定） 
>
>   2. 信息：消除不确定性的事物 
>
>      &emsp;&emsp;&emsp;调整概率
>
>      &emsp;&emsp;&emsp;排除干扰 
>
>      &emsp;&emsp;&emsp;确定情况（eg:瓜农说，保熟包甜）
>
>   3. 噪音：不能消除某人对某件事情不确定性的事物 
>
>   4. 数据：噪音+信息 
>
> 
>
> + 熵如何量化:
>
>   1. 参照一个不确定的事件作为单位，我的不确定性相当于抛硬币的不确定性，50%正，50%反，相当于猜一次抛硬币的不确定性，记为1bit。抛硬币次数与结果不确定性呈指数关系。（eg: &nbsp;硬币：1，结果2； 硬币：2，结果4；硬币：3，结果8；硬币：n，结果  $2^n$ ；） 
>
>   2. 等概率均匀分布：$n = \log_2^{m}$ ；8个等概率的不确定情况，相当于抛 3 个硬币，熵为3bit，4个等概率的不确定情况相当于抛 2 个硬币，熵为2bit，假设有m=10种等概率不确定情况乱，那么$10 = 2^n(即n = \log_2^{10})$； 
>
>   3. 每种情况概率不相等，一般分布
>
>      ​										 $$Ent(D) = -\sum\limits_{k=1}^{|y|}p_k\log_2^{p_k}.$$
>
> 
>
> + 信息如何量化： 
>
>   ​	eg: 小明不知道选择题ABCD 哪个选项时的<font color = sapphire>熵：</font><font color = blue >2</font> 
>
>   ​		  告诉小明 C 有**一半概率正确**提供的**信息**： <font color = green>2 - 1.79</font> = <font color = sienna>0.21</font> （**即信息增益**）
>
>   ​		  小明知道 C 有一半概率 <font color = purple>后</font>的<font color = green>不确定性（熵）</font> ： <font color = sienna>1.79</font>
>
>   ​		  ($Ent(D) = \frac{1}{6}\cdot\log_2^6+\frac{1}{6}\cdot\log_2^6+\frac{1}{2}\cdot\log_2^2+\frac{1}{6}\cdot\log_2^6 = 1.79$)

> **参考资料：** 
>
> + hands on machine learning 2nd 
> + 西瓜书 
> +  [知乎： 决策树算法的python实现-黄耀鹏](https://zhuanlan.zhihu.com/p/20794583) 
> + 知乎： YJango视频

