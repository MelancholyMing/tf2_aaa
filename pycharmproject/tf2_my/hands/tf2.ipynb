{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 5], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1,5],dtype = tf.int64)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(a.dtype)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将numpy的数据类型转换为tensor数据类型 \n",
    "###### tf.convert_to_tensor(数据名，dtype=数据类型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量（tensor）：多维数组（列表）阶：张量的维数\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>维数</th>\n",
       "      <th>阶</th>\n",
       "      <th>名字</th>\n",
       "      <th>例子</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-D</td>\n",
       "      <td>0</td>\n",
       "      <td>标量(scalar)</td>\n",
       "      <td>s=1 2 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-D</td>\n",
       "      <td>1</td>\n",
       "      <td>向量(vector)</td>\n",
       "      <td>v=[1,2,3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-D</td>\n",
       "      <td>2</td>\n",
       "      <td>矩阵（matrix）</td>\n",
       "      <td>m=[[1,2,3],[4,5,6],[7,8,9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3-D</td>\n",
       "      <td>3</td>\n",
       "      <td>张量（tensor）</td>\n",
       "      <td>[[[n个]]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    维数  阶          名字                           例子\n",
       "0  0-D  0  标量(scalar)                      s=1 2 3\n",
       "1  1-D  1  向量(vector)                    v=[1,2,3]\n",
       "2  2-D  2  矩阵（matrix）  m=[[1,2,3],[4,5,6],[7,8,9]]\n",
       "3  3-D  3  张量（tensor）                     [[[n个]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"张量（tensor）：多维数组（列表）阶：张量的维数\")\n",
    "pd.DataFrame({'维数':[\"0-D\",\"1-D\",\"2-D\",\"3-D\"],\"阶\":[0,1,2,3],\"名字\":[\"标量(scalar)\",\"向量(vector)\",\"矩阵（matrix）\",\"张量（tensor）\"],\"例子\":[\"s=1 2 3\",\"v=[1,2,3]\",\"m=[[1,2,3],[4,5,6],[7,8,9]]\",\"[[[n个]]]\"]}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(0,5)\n",
    "b = tf.convert_to_tensor(a,dtype=tf.int64)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[9 9]\n",
      " [9 9]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones([2,3]))\n",
    "print(tf.zeros(4))\n",
    "print(tf.fill([2,2],9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成正态分布的随机数，默认均值为 0 ，标准差为 1。\n",
    "##### tf.random.normal(维度，mean=均值， stddev=标准差)\n",
    "## 生成截断式正态分布的随机数\n",
    "##### tf.random.truncated_normal(维度，mean=均值，stddev=标准差)\n",
    "##### 在tf.truncated_normal中如果随机生成数据的取值在（$\\mu-2\\delta$,$\\mu+2\\delta$）之外，则重新进行生成，保证了生成值在均值附近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.41020828 -1.2745864 ]\n",
      " [ 0.22611707  0.23922771]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.8493935  -1.3501537 ]\n",
      " [-0.24296522 -0.5131544 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d1 = tf.random.normal([2,2],mean=0.5,stddev=1)\n",
    "print(d1)\n",
    "d2 = tf.random.truncated_normal([2,2],mean=0.5,stddev=1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成均匀分布随机数 [minval,maxval]\n",
    "###### tf.random.uniform(维度，minval=最小值，maxval=最大值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.29089904 0.7256893 ]\n",
      " [0.04581022 0.9660255 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d3 = tf.random.uniform([2,2],minval=0,maxval=1)\n",
    "print(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 强制tensor转换为该数据类型\n",
    "# tf.cast(张量名，dtype=数据类型)\n",
    "# 计算张量维度上元素的最小值 tf.reduce_min(张量名)\n",
    "# 计算张量维度上元素的最大值 tf.redue_max(张量名)\n",
    "x1 = tf.constant([1.,2.,3.],dtype= tf.float64)\n",
    "print(x1)\n",
    "x2 = tf.cast(x1,tf.int32)\n",
    "print(x2)\n",
    "print(tf.reduce_min(x2),tf.reduce_max(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 2 3]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor([6 7], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1,2,3],[2,2,3]])\n",
    "print(x)\n",
    "print(tf.reduce_mean(x))\n",
    "print(tf.reduce_sum(x,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 四则运算\n",
    "a = tf.ones([1,3])\n",
    "b= tf.fill([1,3],3.)\n",
    "print(a)\n",
    "print(b)\n",
    "print(tf.add(a,b))\n",
    "print(tf.subtract(a,b))\n",
    "print(tf.multiply(a,b))\n",
    "print(tf.divide(b,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.fill([1,2],3.)\n",
    "print(a)\n",
    "print(tf.pow(a,3))\n",
    "print(tf.square(a))\n",
    "print(tf.sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones([3,2])\n",
    "b = tf.fill([2,3],3.)\n",
    "print(tf.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# tf.data.Dataset.from_tensor_slices（（输入特征，标签））\n",
    "# 切分传入张量的第一位都，生成输入特征/标签对，构建数据集（Numpy和Tensor格式都支持）\n",
    "features = tf.constant([12,23,10,17])\n",
    "labels = tf.constant([0,1,1,0])\n",
    "dataset_ = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "print(dataset_)\n",
    "for element in dataset_:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.GradientTape\n",
    "# with 结构记录计算过程，gradient求出张量的梯度\n",
    "# with GradientTape() as tape:\n",
    "#    若干计算过程\n",
    "#grad = tape.gradient(函数，对谁求导）\n",
    "with tf.GradientTape() as tape:\n",
    "    w = tf.Variable(tf.constant(3.0))\n",
    "    loss = tf.pow(w,2)\n",
    "grad = tape.gradient(loss,w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.onr_hot(待转换数据，几分类)\n",
    "classes = 3\n",
    "labels = tf.constant([1,0,2])\n",
    "output = tf.one_hot(labels,depth = classes)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.nn.softmax  \n",
    "$Softmax(y_i)=\\frac{e^{y_i}}{\\sum\\limits_{j=0}^{n}e^{y_i}}$  \n",
    "\n",
    "使输出符合概率分布\n",
    "当 n 分类的 n 个输出 $(y_0,y_1,\\cdots,y_{n-1})$ 通过softmax()函数，便符合概率分布。  \n",
    "$\\forall x\\;P(X=x)\\in[0,1]且\\sum\\limits_xP(X=x)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after softmax , y_pro is : tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([1.01,2.01,-0.66])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "print(\"after softmax , y_pro is :\",y_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "# a.assign_sub(a要自减的内容)\n",
    "# 更新参数的值并返回，调用assign_sub之前，先用tf.Variable定义变量W为可训练（可自更新）\n",
    "w = tf.Variable(4)\n",
    "w.assign_sub(1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [6 5 4]\n",
      " [8 7 2]]\n",
      "tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# tf.argmax(张量名，axis=操作轴)返回张量沿指定维度最大值的索引\n",
    "import numpy as np\n",
    "test = np.array([[1,2,3],[2,3,4],[6,5,4],[8,7,2]])\n",
    "print(test)\n",
    "print(tf.argmax(test,axis=0))\n",
    "print(tf.argmax(test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data from dataset:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y_data from dataset:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "x_data add index:\n",
      "      花儿长度  花萼宽度  花瓣长度  花瓣宽度\n",
      "0         5.1       3.5       1.4       0.2\n",
      "1         4.9       3.0       1.4       0.2\n",
      "2         4.7       3.2       1.3       0.2\n",
      "3         4.6       3.1       1.5       0.2\n",
      "4         5.0       3.6       1.4       0.2\n",
      "..        ...       ...       ...       ...\n",
      "145       6.7       3.0       5.2       2.3\n",
      "146       6.3       2.5       5.0       1.9\n",
      "147       6.5       3.0       5.2       2.0\n",
      "148       6.2       3.4       5.4       2.3\n",
      "149       5.9       3.0       5.1       1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "x_data add a column:\n",
      "      花儿长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
      "0         5.1       3.5       1.4       0.2     0\n",
      "1         4.9       3.0       1.4       0.2     0\n",
      "2         4.7       3.2       1.3       0.2     0\n",
      "3         4.6       3.1       1.5       0.2     0\n",
      "4         5.0       3.6       1.4       0.2     0\n",
      "..        ...       ...       ...       ...   ...\n",
      "145       6.7       3.0       5.2       2.3     2\n",
      "146       6.3       2.5       5.0       1.9     2\n",
      "147       6.5       3.0       5.2       2.0     2\n",
      "148       6.2       3.4       5.4       2.3     2\n",
      "149       5.9       3.0       5.1       1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "print(\"x_data from dataset:\\n\",x_data)\n",
    "print(\"y_data from dataset:\\n\",y_data)\n",
    "x_data = DataFrame(x_data,columns=[\"花儿长度\",\"花萼宽度\",\"花瓣长度\",\"花瓣宽度\"])\n",
    "pd.set_option(\"display.unicode.east_asian_width\",True)  # 设置列名对其\n",
    "print('x_data add index:\\n',x_data)\n",
    "\n",
    "x_data[\"类别\"] = y_data\n",
    "print(\"x_data add a column:\\n\",x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# 读入数据集\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集乱序\n",
    "np.random.seed(116)  # 使用相同seed，使输入特征/标签一一对应\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集测试集划分\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据类型转换\n",
    "x_train = tf.cast(x_train,tf.float32)\n",
    "x_test = tf.cast(x_test,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 4), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# 配成【输入特征，标签】对，每次喂入一个batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络中所有可训练参数\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4,3],stddev=0.1,seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3],stddev=0.1,seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss:0.04872824624180794\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 1, loss:0.04724756255745888\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 2, loss:0.04610912501811981\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 3, loss:0.04506583511829376\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 4, loss:0.04417487606406212\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 5, loss:0.04346447065472603\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 6, loss:0.042918238788843155\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 7, loss:0.042492225766181946\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 8, loss:0.04214188829064369\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 9, loss:0.04183562472462654\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 10, loss:0.041554585099220276\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 11, loss:0.04128824919462204\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 12, loss:0.04103081673383713\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 13, loss:0.040779076516628265\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 14, loss:0.04053123667836189\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 15, loss:0.04028629511594772\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 16, loss:0.04004373773932457\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 17, loss:0.039803288877010345\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 18, loss:0.0395648367702961\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 19, loss:0.03932837024331093\n",
      "test_acc: 0.16666666666666666\n",
      "-------------------------\n",
      "Epoch 20, loss:0.039093922823667526\n",
      "test_acc: 0.26666666666666666\n",
      "-------------------------\n",
      "Epoch 21, loss:0.038861557841300964\n",
      "test_acc: 0.3333333333333333\n",
      "-------------------------\n",
      "Epoch 22, loss:0.038631368428468704\n",
      "test_acc: 0.4\n",
      "-------------------------\n",
      "Epoch 23, loss:0.038403451442718506\n",
      "test_acc: 0.5\n",
      "-------------------------\n",
      "Epoch 24, loss:0.038177892565727234\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 25, loss:0.03795479238033295\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 26, loss:0.03773424029350281\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 27, loss:0.03751632198691368\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 28, loss:0.03730110824108124\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 29, loss:0.03708868473768234\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 30, loss:0.03687910735607147\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 31, loss:0.036672428250312805\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 32, loss:0.03646869957447052\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 33, loss:0.0362679548561573\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 34, loss:0.036070242524147034\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 35, loss:0.03587557375431061\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 36, loss:0.03568396717309952\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 37, loss:0.03549543395638466\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 38, loss:0.03530997037887573\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 39, loss:0.035127587616443634\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 40, loss:0.03494826331734657\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 41, loss:0.034771986305713654\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 42, loss:0.03459874540567398\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 43, loss:0.03442849963903427\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 44, loss:0.03426123410463333\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 45, loss:0.034096911549568176\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 46, loss:0.03393549472093582\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 47, loss:0.033776942640542984\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 48, loss:0.03362121805548668\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 49, loss:0.033468276262283325\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 50, loss:0.03331806883215904\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 51, loss:0.03317055106163025\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 52, loss:0.03302567079663277\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 53, loss:0.03288338705897331\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 54, loss:0.032743632793426514\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 55, loss:0.032606374472379684\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 56, loss:0.03247154504060745\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 57, loss:0.032339099794626236\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 58, loss:0.03220898658037186\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 59, loss:0.03208114579319954\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 60, loss:0.0319555401802063\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 61, loss:0.03183210268616676\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 62, loss:0.03171078860759735\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 63, loss:0.03159154951572418\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 64, loss:0.03147432580590248\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 65, loss:0.031359076499938965\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 66, loss:0.031245755031704903\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 67, loss:0.031134307384490967\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 68, loss:0.03102468140423298\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 69, loss:0.030916843563318253\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 70, loss:0.03081073984503746\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 71, loss:0.03070632927119732\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 72, loss:0.030603565275669098\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 73, loss:0.030502405017614365\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 74, loss:0.030402816832065582\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 75, loss:0.030304742977023125\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 76, loss:0.0302081611007452\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 77, loss:0.030113015323877335\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 78, loss:0.030019283294677734\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 79, loss:0.02992691658437252\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 80, loss:0.029835887253284454\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 81, loss:0.029746154323220253\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 82, loss:0.02965768799185753\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 83, loss:0.029570456594228745\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 84, loss:0.029484424740076065\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 85, loss:0.029399564489722252\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 86, loss:0.02931583672761917\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 87, loss:0.02923322468996048\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 88, loss:0.029151691123843193\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 89, loss:0.029071208089590073\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 90, loss:0.02899175137281418\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 91, loss:0.02891329862177372\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, loss:0.028835812583565712\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 93, loss:0.02875928021967411\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 94, loss:0.028683671727776527\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 95, loss:0.028608964756131172\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 96, loss:0.028535135090351105\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 97, loss:0.028462162241339684\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 98, loss:0.028390023857355118\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 99, loss:0.028318703174591064\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 100, loss:0.028248177841305733\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 101, loss:0.028178419917821884\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 102, loss:0.02810942381620407\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 103, loss:0.028041167184710503\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 104, loss:0.027973629534244537\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 105, loss:0.027906788513064384\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 106, loss:0.027840634807944298\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 107, loss:0.027775155380368233\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 108, loss:0.02771032601594925\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 109, loss:0.027646133676171303\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 110, loss:0.0275825634598732\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 111, loss:0.027519606053829193\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 112, loss:0.027457240968942642\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 113, loss:0.0273954588919878\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 114, loss:0.027334239333868027\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 115, loss:0.027273578569293022\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 116, loss:0.027213461697101593\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 117, loss:0.027153870090842247\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 118, loss:0.027094801887869835\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 119, loss:0.027036236599087715\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 120, loss:0.02697817236185074\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 121, loss:0.026920590549707413\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 122, loss:0.02686348371207714\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 123, loss:0.026806842535734177\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 124, loss:0.026750653982162476\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 125, loss:0.02669491432607174\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 126, loss:0.026639606803655624\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 127, loss:0.026584729552268982\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 128, loss:0.026530267670750618\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 129, loss:0.026476219296455383\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 130, loss:0.026422569528222084\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 131, loss:0.026369312778115273\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 132, loss:0.0263164434581995\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 133, loss:0.026263954117894173\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 134, loss:0.02621183544397354\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 135, loss:0.026160074397921562\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 136, loss:0.026108672842383385\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 137, loss:0.026057623326778412\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 138, loss:0.02600691467523575\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 139, loss:0.025956545025110245\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 140, loss:0.025906503200531006\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 141, loss:0.02585678920149803\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 142, loss:0.025807389989495277\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 143, loss:0.025758305564522743\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 144, loss:0.025709528475999832\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 145, loss:0.025661054998636246\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 146, loss:0.02561287395656109\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 147, loss:0.02556498534977436\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 148, loss:0.025517385452985764\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 149, loss:0.02547006495296955\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 150, loss:0.025423025712370872\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 151, loss:0.025376252830028534\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 152, loss:0.025329750031232834\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 153, loss:0.025283511728048325\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 154, loss:0.02523753046989441\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 155, loss:0.02519180439412594\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 156, loss:0.025146331638097763\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 157, loss:0.025101104751229286\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 158, loss:0.02505611814558506\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 159, loss:0.025011373683810234\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 160, loss:0.02496686391532421\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 161, loss:0.024922586977481842\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 162, loss:0.02487853914499283\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 163, loss:0.024834714829921722\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 164, loss:0.024791114032268524\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 165, loss:0.024747733026742935\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 166, loss:0.024704568088054657\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 167, loss:0.024661613628268242\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 168, loss:0.02461887151002884\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 169, loss:0.024576334282755852\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 170, loss:0.02453400008380413\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 171, loss:0.024491868913173676\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 172, loss:0.024449937045574188\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 173, loss:0.02440820075571537\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 174, loss:0.024366656318306923\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 175, loss:0.024325303733348846\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 176, loss:0.02428414113819599\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 177, loss:0.02424316108226776\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 178, loss:0.024202369153499603\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 179, loss:0.024161754176020622\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 180, loss:0.024121321737766266\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 181, loss:0.024081068113446236\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 182, loss:0.024040985852479935\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 183, loss:0.024001076817512512\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 184, loss:0.02396133914589882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 185, loss:0.023921770974993706\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 186, loss:0.023882372304797173\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 187, loss:0.023843135684728622\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 188, loss:0.023804061114788055\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 189, loss:0.02376515232026577\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 190, loss:0.02372639998793602\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 191, loss:0.023687805980443954\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 192, loss:0.023649372160434723\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 193, loss:0.02361108921468258\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 194, loss:0.02357296086847782\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 195, loss:0.023534981533885002\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 196, loss:0.02349715679883957\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 197, loss:0.023459473624825478\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 198, loss:0.023421943187713623\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 199, loss:0.02338455803692341\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 200, loss:0.023347316309809685\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 201, loss:0.023310218006372452\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 202, loss:0.02327326312661171\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 203, loss:0.02323644421994686\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 204, loss:0.023199763149023056\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 205, loss:0.023163223639130592\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 206, loss:0.023126820102334023\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 207, loss:0.023090550675988197\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 208, loss:0.023054415360093117\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 209, loss:0.02301841229200363\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 210, loss:0.022982541471719742\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 211, loss:0.022946801036596298\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 212, loss:0.02291118912398815\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 213, loss:0.022875705733895302\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 214, loss:0.02284035086631775\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 215, loss:0.022805120795965195\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 216, loss:0.022770019248127937\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 217, loss:0.02273503690958023\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 218, loss:0.022700181230902672\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 219, loss:0.022665448486804962\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 220, loss:0.022630833089351654\n",
      "test_acc: 0.5333333333333333\n",
      "-------------------------\n",
      "Epoch 221, loss:0.022596342489123344\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 222, loss:0.022561969235539436\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 223, loss:0.02252771519124508\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 224, loss:0.022493578493595123\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 225, loss:0.02245955727994442\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 226, loss:0.022425655275583267\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 227, loss:0.02239186502993107\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 228, loss:0.02235819213092327\n",
      "test_acc: 0.5666666666666667\n",
      "-------------------------\n",
      "Epoch 229, loss:0.022324630990624428\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 230, loss:0.022291183471679688\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 231, loss:0.0222578477114439\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 232, loss:0.02222462184727192\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 233, loss:0.02219150774180889\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 234, loss:0.022158503532409668\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 235, loss:0.02212560921907425\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 236, loss:0.022092822939157486\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 237, loss:0.022060144692659378\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 238, loss:0.022027574479579926\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 239, loss:0.02199510484933853\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 240, loss:0.02196274697780609\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 241, loss:0.021930493414402008\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 242, loss:0.021898342296481133\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 243, loss:0.021866295486688614\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 244, loss:0.021834351122379303\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 245, loss:0.02180250734090805\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 246, loss:0.021770769730210304\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 247, loss:0.021739132702350616\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 248, loss:0.021707594394683838\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 249, loss:0.021676158532500267\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 250, loss:0.021644821390509605\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 251, loss:0.021613581106066704\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 252, loss:0.02158244140446186\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 253, loss:0.02155139669775963\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 254, loss:0.021520454436540604\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 255, loss:0.02148960717022419\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 256, loss:0.021458853036165237\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 257, loss:0.021428197622299194\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 258, loss:0.021397637203335762\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 259, loss:0.02136716991662979\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 260, loss:0.02133679948747158\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 261, loss:0.02130652219057083\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 262, loss:0.021276334300637245\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 263, loss:0.02124624513089657\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 264, loss:0.021216247230768204\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 265, loss:0.021186338737607002\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 266, loss:0.021156523376703262\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 267, loss:0.021126797422766685\n",
      "test_acc: 0.6\n",
      "-------------------------\n",
      "Epoch 268, loss:0.02109716273844242\n",
      "test_acc: 0.6333333333333333\n",
      "-------------------------\n",
      "Epoch 269, loss:0.02106761932373047\n",
      "test_acc: 0.6333333333333333\n",
      "-------------------------\n",
      "Epoch 270, loss:0.02103816717863083\n",
      "test_acc: 0.6333333333333333\n",
      "-------------------------\n",
      "Epoch 271, loss:0.021008798852562904\n",
      "test_acc: 0.6333333333333333\n",
      "-------------------------\n",
      "Epoch 272, loss:0.02097952365875244\n",
      "test_acc: 0.6333333333333333\n",
      "-------------------------\n",
      "Epoch 273, loss:0.02095033787190914\n",
      "test_acc: 0.6333333333333333\n",
      "-------------------------\n",
      "Epoch 274, loss:0.020921237766742706\n",
      "test_acc: 0.6666666666666666\n",
      "-------------------------\n",
      "Epoch 275, loss:0.020892225205898285\n",
      "test_acc: 0.6666666666666666\n",
      "-------------------------\n",
      "Epoch 276, loss:0.020863302052021027\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 277, loss:0.020834460854530334\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 278, loss:0.020805712789297104\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 279, loss:0.02077704668045044\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 280, loss:0.02074846439063549\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 281, loss:0.020719967782497406\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 282, loss:0.020691560581326485\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 283, loss:0.02066323533654213\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 284, loss:0.02063499391078949\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 285, loss:0.020606834441423416\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 286, loss:0.020578762516379356\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 287, loss:0.020550770685076714\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 288, loss:0.020522862672805786\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 289, loss:0.020495034754276276\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 290, loss:0.02046729065477848\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 291, loss:0.02043962851166725\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 292, loss:0.02041204832494259\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 293, loss:0.020384550094604492\n",
      "test_acc: 0.7\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294, loss:0.020357126370072365\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 295, loss:0.02032979018986225\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 296, loss:0.020302530378103256\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 297, loss:0.020275350660085678\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 298, loss:0.020248251035809517\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 299, loss:0.020221227779984474\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 300, loss:0.02019428461790085\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 301, loss:0.02016742154955864\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 302, loss:0.020140638574957848\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 303, loss:0.020113926380872726\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 304, loss:0.02008729614317417\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 305, loss:0.020060742273926735\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 306, loss:0.020034266635775566\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 307, loss:0.020007865503430367\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 308, loss:0.019981540739536285\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 309, loss:0.019955294206738472\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 310, loss:0.01992912031710148\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 311, loss:0.019903024658560753\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 312, loss:0.019876999780535698\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 313, loss:0.01985105499625206\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 314, loss:0.019825182855129242\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 315, loss:0.019799381494522095\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 316, loss:0.019773656502366066\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 317, loss:0.019748006016016006\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 318, loss:0.019722428172826767\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 319, loss:0.019696924835443497\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 320, loss:0.019671492278575897\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 321, loss:0.019646132364869118\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 322, loss:0.019620846956968307\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 323, loss:0.019595632329583168\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 324, loss:0.01957049034535885\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 325, loss:0.01954541727900505\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 326, loss:0.019520418718457222\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 327, loss:0.019495487213134766\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 328, loss:0.01947063021361828\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 329, loss:0.019445843994617462\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 330, loss:0.01942112296819687\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 331, loss:0.019396476447582245\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 332, loss:0.01937190070748329\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 333, loss:0.01934739202260971\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 334, loss:0.01932295225560665\n",
      "test_acc: 0.7\n",
      "-------------------------\n",
      "Epoch 335, loss:0.019298583269119263\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 336, loss:0.019274281337857246\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 337, loss:0.01925004832446575\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 338, loss:0.01922588422894478\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 339, loss:0.019201787188649178\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 340, loss:0.01917775720357895\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 341, loss:0.019153796136379242\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 342, loss:0.019129902124404907\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 343, loss:0.019106078892946243\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 344, loss:0.019082317128777504\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 345, loss:0.019058624282479286\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 346, loss:0.01903499662876129\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 347, loss:0.01901143789291382\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 348, loss:0.01898794248700142\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 349, loss:0.018964512273669243\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 350, loss:0.01894114725291729\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 351, loss:0.018917851150035858\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 352, loss:0.0188946183770895\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 353, loss:0.018871448934078217\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 354, loss:0.018848346546292305\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 355, loss:0.018825305625796318\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 356, loss:0.018802331760525703\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 357, loss:0.018779419362545013\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 358, loss:0.018756575882434845\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 359, loss:0.018733788281679153\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 360, loss:0.018711069598793983\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 361, loss:0.01868841052055359\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 362, loss:0.018665814772248268\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 363, loss:0.01864328421652317\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 364, loss:0.018620813265442848\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 365, loss:0.0185984056442976\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 366, loss:0.018576063215732574\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 367, loss:0.018553778529167175\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 368, loss:0.01853155717253685\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 369, loss:0.01850939728319645\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 370, loss:0.018487296998500824\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 371, loss:0.018465260043740273\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 372, loss:0.018443282693624496\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 373, loss:0.018421366810798645\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 374, loss:0.01839951053261757\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 375, loss:0.01837771385908127\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 376, loss:0.018355976790189743\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 377, loss:0.01833430305123329\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 378, loss:0.018312685191631317\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 379, loss:0.018291128799319267\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 380, loss:0.018269630149006844\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 381, loss:0.018248192965984344\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 382, loss:0.018226809799671173\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 383, loss:0.018205489963293076\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 384, loss:0.018184226006269455\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 385, loss:0.01816302165389061\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 386, loss:0.01814187504351139\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 387, loss:0.018120788037776947\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 388, loss:0.01809975691139698\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 389, loss:0.01807878352701664\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 390, loss:0.018057866021990776\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 391, loss:0.018037008121609688\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 392, loss:0.018016207963228226\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 393, loss:0.01799546368420124\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 394, loss:0.017974773421883583\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 395, loss:0.017954140901565552\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 396, loss:0.017933564260601997\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 397, loss:0.01791304722428322\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 398, loss:0.017892582342028618\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 399, loss:0.017872173339128494\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 400, loss:0.017851820215582848\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 401, loss:0.017831522971391678\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 402, loss:0.017811277881264687\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 403, loss:0.01779109239578247\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 404, loss:0.017770959064364433\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 405, loss:0.017750881612300873\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 406, loss:0.01773085817694664\n",
      "test_acc: 0.7333333333333333\n",
      "-------------------------\n",
      "Epoch 407, loss:0.017710886895656586\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 408, loss:0.017690973356366158\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 409, loss:0.017671111971139908\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 410, loss:0.017651302739977837\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 411, loss:0.017631549388170242\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 412, loss:0.017611848190426826\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 413, loss:0.01759220100939274\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 414, loss:0.01757260598242283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 415, loss:0.017553064972162247\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 416, loss:0.017533577978610992\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 417, loss:0.017514139413833618\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 418, loss:0.01749475486576557\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 419, loss:0.017475422471761703\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 420, loss:0.017456144094467163\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 421, loss:0.017436914145946503\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 422, loss:0.01741773821413517\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 423, loss:0.017398616299033165\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 424, loss:0.01737954281270504\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 425, loss:0.017360519617795944\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 426, loss:0.017341546714305878\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 427, loss:0.01732262596487999\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 428, loss:0.01730375736951828\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 429, loss:0.01728494092822075\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 430, loss:0.017266172915697098\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 431, loss:0.017247453331947327\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 432, loss:0.017228785902261734\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 433, loss:0.01721016690135002\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 434, loss:0.017191598191857338\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 435, loss:0.017173079773783684\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 436, loss:0.01715461164712906\n",
      "test_acc: 0.7666666666666667\n",
      "-------------------------\n",
      "Epoch 437, loss:0.017136190086603165\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 438, loss:0.01711782068014145\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 439, loss:0.017099497839808464\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 440, loss:0.01708122342824936\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 441, loss:0.017062999308109283\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 442, loss:0.01704482175409794\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 443, loss:0.017026696354150772\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 444, loss:0.017008617520332336\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 445, loss:0.01699058525264263\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 446, loss:0.016972603276371956\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 447, loss:0.016954666003584862\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 448, loss:0.016936779022216797\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 449, loss:0.016918936744332314\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 450, loss:0.01690114475786686\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 451, loss:0.016883399337530136\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 452, loss:0.016865698620676994\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 453, loss:0.016848048195242882\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 454, loss:0.0168304406106472\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 455, loss:0.01681288331747055\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 456, loss:0.01679537072777748\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 457, loss:0.016777902841567993\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 458, loss:0.016760483384132385\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 459, loss:0.01674310863018036\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 460, loss:0.016725782305002213\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 461, loss:0.01670849695801735\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 462, loss:0.016691260039806366\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 463, loss:0.016674067825078964\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 464, loss:0.016656920313835144\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 465, loss:0.016639819368720055\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 466, loss:0.016622761264443398\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 467, loss:0.01660575158894062\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 468, loss:0.016588784754276276\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 469, loss:0.016571858897805214\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 470, loss:0.01655498333275318\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 471, loss:0.016538148745894432\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 472, loss:0.016521356999874115\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 473, loss:0.016504613682627678\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 474, loss:0.016487909480929375\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 475, loss:0.016471249982714653\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 476, loss:0.016454637050628662\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 477, loss:0.016438063234090805\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 478, loss:0.01642153598368168\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 479, loss:0.016405049711465836\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 480, loss:0.016388608142733574\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 481, loss:0.016372211277484894\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 482, loss:0.016355853527784348\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 483, loss:0.016339540481567383\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 484, loss:0.0163232684135437\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 485, loss:0.0163070410490036\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 486, loss:0.016290854662656784\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 487, loss:0.0162747111171484\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 488, loss:0.01625860668718815\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 489, loss:0.01624254509806633\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 490, loss:0.016226526349782944\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 491, loss:0.01621054857969284\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 492, loss:0.01619461178779602\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 493, loss:0.016178717836737633\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 494, loss:0.016162864863872528\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 495, loss:0.016147051006555557\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 496, loss:0.01613127999007702\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 497, loss:0.016115548089146614\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 498, loss:0.016099857166409492\n",
      "test_acc: 0.8\n",
      "-------------------------\n",
      "Epoch 499, loss:0.016084207221865654\n",
      "test_acc: 0.8\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxNElEQVR4nO3deXxV1bn/8c+TOSQhIQNTAiQQEAEFJOIEKjhctFZsqxXrVdvaUn9qW396+6ve1g7etvfa2zpV7dVWKlqttt6qOFsFZ4sEBZkhIJgwZQASAoQheX5/nB08xAAJ5ORk+L5fr/Pi7LXX3udZMZ4na6+91zJ3R0REpKVioh2AiIh0LkocIiLSKkocIiLSKkocIiLSKkocIiLSKkocIiLSKkocIh2ImdWa2eBoxyFyKEoc0uGY2VozOzsKn/uwme0JvrwbX5dG8PPeMLNvhZe5e6q7r4nQ533NzIqDdm00s5fMbEIkPku6NiUOkQP9Ovjybnw9Ge2A2oKZ3QjcBfwK6AMMBO4Hph7BueLaNDjpdJQ4pNMws0Qzu8vMNgSvu8wsMdiXbWbPm9k2M9tiZm+bWUyw74dmtt7MtpvZCjM7q5Wf+7CZ/SJs+0wzKwvbXmtm/2ZmH5tZtZk9aWZJYfunmtkCM6sxs9VmNsXMfglMBO4NegD3BnXdzAqD9+lm9oiZVZjZOjP7cVibvm5m75jZb8xsq5l9YmbnHST+dOA24Dp3/7u773D3ve7+nLv/oBVt/KGZfQzsCN4/1eRz7jaze8Jifyjo2aw3s1+YWWxrfu7ScekvB+lMfgScDIwBHHgW+DFwK3ATUAbkBHVPBtzMjgGuB0509w1mlg9E4gvsq8AUoA54F/g68D9mNh54BLgYeB3oB6S5+8tmdhrwZ3f/40HO+TsgHRgMZAGvAhuBh4L9JwEzgWxgOvCQmeX65+cROgVIAp4+yjZeBnwBqAR6Az81szR33x4kha8CXwrqPgyUA4VACvA8UAo8cJQxSAegHod0JpcDt7l7ubtXAD8Hrgj27SX0pTwo+Gv67eALtB5IBEaYWby7r3X31Yf4jH8Lei3bzKyyFbHd4+4b3H0L8Byh5AZwNTDD3f/h7g3uvt7dlx/uZMEX8TTgFnff7u5rgd+GtRdgnbv/wd3rCSWQfoQuQzWVBVS6+75WtKc597h7qbvvcvd1wId8ligmAzvd/Z9m1gc4H7gh6N2UA3cG7ZEuQIlDOpP+wLqw7XVBGcB/AyXAq2a2xsxuBnD3EuAG4GdAuZk9YWb9ObjfuHtG8MpuRWybwt7vBFKD9wOAQyWqg8kG4vl8e3Ob+0x33xm8TeXzqoDsNhibKG2y/TihXgjA14JtgEGEYt/YmIQJ9TR6H+XnSwehxCGdyQZCX0qNBgZlBH+V3+Tug4ELgRsbxzLc/XF3nxAc68DtrfzcHUCPsO2+rTi2FBhykH2Hmpq6klAvqml717fisxu9D+wGLjpEnZa0sWm8fwPONLM8Qj2PxsRRGnxedlgS7unuI48gdumAlDiko4o3s6SwVxzwF+DHZpZjZtnAT4A/A5jZBWZWaGYGVBO6RNVgZseY2eRgEL0O2AU0tDKWBcD5ZpZpZn0J9WBa6iHgG2Z2lpnFmFmumQ0P9m0mNH7xOcHlp78CvzSzNDMbBNzY2N7WcPdqQj+r+8zsIjPrYWbxZnaemf36SNsYXC58A/gT8Im7LwvKNxIaj/mtmfUM2j3EzM5obezSMSlxSEf1IqEv+cbXz4BfAMXAx8AiQtfYG+8EGgq8BtQS+gv7fnefQ2h8478I/QW/idDlkltaGcujwEJgLaEvxBbfouvuHwDfIHSNvxp4k896EXcDFwd3Rd3TzOHfJdQTWAO8Q+gv+hmtjL0xjt8SSjw/BioI9QquB54JqhxpGx8Hzuaz3kajK4EEYCmwFXiK0BiMdAGmhZxERKQ11OMQEZFWUeIQEZFWUeIQEZFWiWjiCKZWWGFmJY331TfZnxhMz1BiZnODp3oxs3wz2xVM07DAzP4n7JhxZrYoOOae4C4aERFpJxGbciR48vU+4BxCU0HMM7NZ7r40rNrVwFZ3LzSzaYTur2+cjXS1u49p5tS/B74NzCV0580U4KVDxZKdne35+flH0RoRke5n/vz5le6e07Q8knNVjQdKGqeINrMnCM3EGZ44phK6zRJCt+vde6gehJn1A3q6+z+D7UcIPdR0yMSRn59PcXHxkbVCRKSbMrN1zZVH8lJVLgdOUVDGgdMlHFAnmEenmtC8OgAFZvaRmb1pZhPD6peFHd/cOUVEJII66uy4G4GB7l5lZuOAZ8ysVdMVmNl0QjOGMnDgwAiEKCLSPUWyx7Ge0ARvjfL4/Dw7++sEU0qkA1XuvtvdqwDcfT6hSeKGBfXzDnNOguMedPcidy/KyfncJToRETlCkexxzAOGmlkBoS/3aYRm0Aw3C7iK0BQRFwOz3d3NLAfY4u71Flp/eSiwxt23WGgxnJMJDY5fSWjNAhGRNrV3717Kysqoq6uLdigRl5SURF5eHvHx8S2qH7HE4e77zOx64BVCC+fMcPclZnYbUOzuswhNAPeomZUAW/hsvv7TgdvMbC+hCemuCdY5ALiW0CIxyYQGxQ85MC4iciTKyspIS0sjPz+frnzXv7tTVVVFWVkZBQUFLTomomMc7v4ioVtmw8t+Eva+DrikmeP+F/jfg5yzGBjVtpGKiByorq6uyycNADMjKyuLioqKFh+jJ8dFRA6iqyeNRq1tpxLHIcx8by2zFm6IdhgiIh2KEsch/OWDT3lOiUNE5ABKHIfQMzme6p17ox2GiEiHosRxCBnJ8VTvUuIQkeh54IEHuO6666IdxgGUOA4hXYlDRKJs0aJFHHfccdEO4wBKHIeQ0SOebbv2RDsMEenGPv74488ljuXLlzN58mTGjBnD2WefTWVlJQAzZ85k3LhxHH/88UyYMOGgZUero85V1SGkJ8dTt7eBur31JMXHRjscEYmSnz+3hKUbatr0nCP69+SnXzz8FHyLFy9m1KjPHl3bvXs3X/nKV3jssccYM2YMt99+O3feeSc333wzt99+OwsWLCAhIYFt27axffv2z5W1BfU4DiG9RwIANbpcJSJRUFpaSlpaGunp6fvLnnnmGSZMmMCYMWMAGDFiBOXl5cTGxrJr1y5uuukmiouLycjIaLasLajHcQjpyaF5W6p37aV3z6QoRyMi0dKSnkEkNDe+sXTp0gPKFi1axIgRI+jRoweLFy/mueeeY/r06XzrW9/i2muvbbbsaClxHEJGWOIQEWlvzY1v5ObmsmDBAgDWrFnDo48+yjvvvMOqVasYOnQo06ZNY+nSpdTV1TVb1haUOA6hV3CpqmqHBshFpP0tWrSIl19+mb/85S8A9OvXj9mzZ/Piiy9y3HHHkZyczIwZM8jKyuKmm27i/fffJyUlhZEjR/KHP/yBa6655nNlbUGJ4xCy00KJo7J2d5QjEZHu6LHHHmu2/Jlnnvlc2cMPP9yisragwfFDyEpJBKByu3ocIiKNlDgOISEuhowe8VTUdv2FXEREWkqJ4zByUhPV4xDpptw92iG0i9a2U4njMHLSEqnQGIdIt5OUlERVVVWXTx6NKwAmJbX8kQMNjh9GdmoiC8u2RTsMEWlneXl5lJWVtWplvM6qcc3xllLiOIyctEQqtqvHIdLdxMfHt3gN7u5Gl6oOIzs1kZ176tmxe1+0QxER6RCUOA4jJy24JVfjHCIigBLHYTUmDl2uEhEJiWjiMLMpZrbCzErM7OZm9iea2ZPB/rlmlt9k/0AzqzWzfwsrW2tmi8xsgZkVRzJ+gOxUPT0uIhIuYonDzGKB+4DzgBHAZWY2okm1q4Gt7l4I3Anc3mT/HcBLzZx+kruPcfeiNg77c9TjEBE5UCR7HOOBEndf4+57gCeAqU3qTAVmBu+fAs4yMwMws4uAT4AlEYzxsLJSEokxKFfiEBEBIps4coHSsO2yoKzZOu6+D6gGsswsFfgh8PNmzuvAq2Y238ymH+zDzWy6mRWbWfHR3IcdG2P0TktiwzZNOyIiAh13cPxnwJ3uXtvMvgnufgKhS2DXmdnpzZ3A3R909yJ3L8rJyTmqYPplJLGxetdRnUNEpKuI5AOA64EBYdt5QVlzdcrMLA5IB6qAk4CLzezXQAbQYGZ17n6vu68HcPdyM3ua0CWxtyLYDvqnJ7N0Y9uuNywi0llFsscxDxhqZgVmlgBMA2Y1qTMLuCp4fzEw20Mmunu+u+cDdwG/cvd7zSzFzNIAzCwFOBdYHME2ANAvPYkN23Z1+TlrRERaImI9DnffZ2bXA68AscAMd19iZrcBxe4+C3gIeNTMSoAthJLLofQBng7Gz+OAx9395Ui1oVG/jGR272tg6869ZKYkRPrjREQ6tIjOVeXuLwIvNin7Sdj7OuCSw5zjZ2Hv1wCj2zbKwxuY2QOAT7fsVOIQkW6vow6Odyj5WaHEsa5qR5QjERGJPiWOFhiQ2QMzWFu5M9qhiIhEnRJHCyTFx9I/PZm16nGIiChxtNTgnBRWlW+PdhgiIlGnxNFCx/brycrNteytb4h2KCIiUaXE0ULH9ktjz74G1lTocpWIdG9KHC00sn86gNYfF5FuT4mjhQpzUslMSeCfq6uiHYqISFQpcbRQTIxxyuAs3ltdpalHRKRbU+JohVMLs9hUU8cnlRrnEJHuS4mjFU4dkg3Au7pcJSLdmBJHK+Rn9WBAZjKvLd0c7VBERKJGiaMVzIwLju/POyWVbNmxJ9rhiIhEhRJHK33x+P7UNzgvLd4Y7VBERKJCiaOVju2XxpCcFJ79aEO0QxERiQoljlYyM758Qh4frN3CmormlkQXEenalDiOwCXj8oiNMZ6cVxrtUERE2p0SxxHo3TOJs4b35qn5ZezZp0kPRaR7UeI4QpeNH0jVjj28vky35opI96LEcYROH5ZDv/Qk/qLLVSLSzShxHKHYGOOSogG8vaqC0i1aUlZEuo+IJg4zm2JmK8ysxMxubmZ/opk9Geyfa2b5TfYPNLNaM/u3lp6zPU07cQAGPDb302iGISLSriKWOMwsFrgPOA8YAVxmZiOaVLsa2OruhcCdwO1N9t8BvNTKc7ab/hnJnDuiL0/O+5S6vfXRCkNEpF1FsscxHihx9zXuvgd4ApjapM5UYGbw/ingLDMzADO7CPgEWNLKc7arK08dxNade3luoR4IFJHuIZKJIxcIHzkuC8qarePu+4BqIMvMUoEfAj8/gnMCYGbTzazYzIorKiqOuBGHc8rgLIb2TmXm+2u1ToeIdAsddXD8Z8Cd7n7Ej2a7+4PuXuTuRTk5OW0XWRNmxpWn5rN4fQ0flW6L2OeIiHQUkUwc64EBYdt5QVmzdcwsDkgHqoCTgF+b2VrgBuDfzez6Fp6z3X15bC5piXHMfG9ttEMREYm4SCaOecBQMyswswRgGjCrSZ1ZwFXB+4uB2R4y0d3z3T0fuAv4lbvf28JztruUxDguLsrjxUUbKd9eF+1wREQiKmKJIxizuB54BVgG/NXdl5jZbWZ2YVDtIUJjGiXAjcAhb6892Dkj1YbWuOLkQeytd574QA8EikjXZt1hQLeoqMiLi4sj/jlXzviAFZtqeOeHk4mP7ajDRyIiLWNm8929qGm5vt3a0FWnDGJzzW5eWbIp2qGIiESMEkcbOvOY3gzK6sFD73wS7VBERCJGiaMNxcYY3zytgI8+3cb8dVuiHY6ISEQocbSxS4rySE+O5w9vqdchIl2TEkcb65EQx+UnDeSVpZtYV7Uj2uGIiLQ5JY4IuOrUfOJijBka6xCRLkiJIwL69EziwtG5/LW4jG0790Q7HBGRNqXEESHfmljArr31WqtDRLocJY4IObZfTyYOzWbme2vZvU9rdYhI16HEEUHTTx9M+fbd/P3DqM/DKCLSZpQ4ImhCYTaj89K5/40S9tU3RDscEZE2ocQRQWbG9ZOHUrplF7O0QqCIdBFKHBF21vDeDO+bxn1zSmho6PoTSopI16fEEWExMcb1kwtZXbGDlzX5oYh0AUoc7eC8Uf0YnJPC72aXaF1yEen0lDjaQWyMce2ZhSzbWMPs5eXRDkdE5KgocbSTqWP6k9crmXteX6Veh4h0akoc7SQ+NobvTR7KwrJqXl26OdrhiIgcMSWOdvTlE3IZnJPCb15ZQb3usBKRTkqJox3FxcZw0znHsKq8lmc+0tPkItI5KXG0s/NG9WVUbk/ufG0le/bpaXIR6XwimjjMbIqZrTCzEjO7uZn9iWb2ZLB/rpnlB+XjzWxB8FpoZl8KO2atmS0K9hVHMv5IiIkxfvAvwynbuosn5mnmXBHpfCKWOMwsFrgPOA8YAVxmZiOaVLsa2OruhcCdwO1B+WKgyN3HAFOAB8wsLuy4Se4+xt2LIhV/JJ0+NJuTCjK55/VVbK/bG+1wRERaJZI9jvFAibuvcfc9wBPA1CZ1pgIzg/dPAWeZmbn7TnffF5QnAV1qJNnM+NEXjqWydg/3zVkd7XBERFolkokjFygN2y4LypqtEySKaiALwMxOMrMlwCLgmrBE4sCrZjbfzKYf7MPNbLqZFZtZcUVFRZs0qC0dn5fBV07IY8Y7n/Bp1c5ohyMi0mIddnDc3ee6+0jgROAWM0sKdk1w9xMIXQK7zsxOP8jxD7p7kbsX5eTktFPUrfP/phxDbIzxqxeXRTsUEZEWi2TiWA8MCNvOC8qarROMYaQDVeEV3H0ZUAuMCrbXB/+WA08TuiTWKfXpmcS1Zw7h5SWbeH911eEPEBHpACKZOOYBQ82swMwSgGnArCZ1ZgFXBe8vBma7uwfHxAGY2SBgOLDWzFLMLC0oTwHOJTSQ3ml9+/TB5GYkc9vzS7XYk4h0ChFLHMGYxPXAK8Ay4K/uvsTMbjOzC4NqDwFZZlYC3Ag03rI7AVhoZgsI9SqudfdKoA/wjpktBD4AXnD3lyPVhvaQFB/Lj75wLMs21vDwe2ujHY6IyGFZd5hwr6ioyIuLO+4jH+7O1TOL+eeaKv5x4xnkZiRHOyQREcxsfnOPPXTYwfHuxMz4+YUjcYefPrtYs+eKSIemxNFBDMjswf89ZyivLSvnlSWaPVdEOq4WJY5gUDomeD/MzC40s/jIhtb9fOO0Aob3TeOnsxZTvVNPlItIx9TSHsdbQJKZ5QKvAlcAD0cqqO4qPjaGX198PJW1e/j5c0uiHY6ISLNamjjM3XcCXwbud/dLgJGRC6v7Oj4vg+smFfL3j9bz8uJN0Q5HRORzWpw4zOwU4HLghaAsNjIhyXcnFzIqtyc/enoRlbW7ox2OiMgBWpo4bgBuAZ4OnsUYDMyJWFTdXHxsDHd8dQzbd+/j3/++SHdZiUiH0qLE4e5vuvuF7n57MEhe6e7fi3Bs3dqwPmn84NxjeHXpZv78z3XRDkdEZL+W3lX1uJn1DKb5WAwsNbMfRDY0uXpCAWcek8N/PL+Mxeurox2OiAjQ8ktVI9y9BrgIeAkoIHRnlURQTIxxx1fHkJmSwPWPf6hFn0SkQ2hp4ogPntu4CJjl7nvpYosrdVSZKQncc9lYSrfu4haNd4hIB9DSxPEAsBZIAd4KZqytiVRQcqDxBZnceM4wnv94I398+5NohyMi3VxLB8fvcfdcdz/fQ9YBkyIcm4T5P2cM4bxRffnPl5bx5sqOt6KhiHQfLR0cTzezOxqXYjWz3xLqfUg7iYkxfnPJaIb1SeO7j3/IJ5U7oh2SiHRTLb1UNQPYDnw1eNUAf4pUUNK8lMQ4/nBlEbExxrdmztN8ViISFS1NHEPc/afuviZ4/RwYHMnApHkDMnvw+38dx6dbdvLtR4up21sf7ZBEpJtpaeLYZWYTGjfM7DRgV2RCksM5eXAWv7lkNB98soWb/rqQhgbdaSUi7SeuhfWuAR4xs/RgeyufrRUuUTB1TC6ba+r41YvL6d0zkZ9cMAIzi3ZYItINtChxuPtCYLSZ9Qy2a8zsBuDjCMYmh/HtiYPZWF3Hn95dS3pyPDecPSzaIYlIN9DSHgcQShhhmzcCd7VpNNIqZsatXxhBza593PXaKhLiYrj2zMJohyUiXVyrEkcTui7SAcTEGL+++Hj21jfw65dXkBAbw7cm6r4FEYmco1lz/LAjsmY2xcxWmFmJmd3czP5EM3sy2D/XzPKD8vFmtiB4LTSzL7X0nN1RbIxxx1dHc96ovvzihWU88v7aaIckIl3YIXscZrad5hOEAcmHOTYWuA84BygD5pnZLHdfGlbtamCruxea2TTgduBSQjPwFrn7PjPrByw0s+eCWA53zm4pLjaGu6eNZe9jH/KTZ5ewc08915wxJNphiUgXdMgeh7unuXvPZl5p7n64y1zjgZLguY89wBPA1CZ1pgIzg/dPAWeZmbn7TnffF5Qn8Vnyask5u62EuBh+/68n8MXR/fmvl5bz65eXa1JEEWlzRzPGcTi5QGnYdhlw0sHqBL2LaiALqDSzkwg9sT4IuCLY35JzdmvxsTHcdekYUhPjuP+N1dTU7eW2C0cRE6MhKRFpG5FMHEfF3ecCI83sWGCmmb3UmuPNbDowHWDgwIERiLDjio0xfvWlUfRMiuOBt9awdcdefvvV0STFa5l4ETl6RzM4fjjrgQFh23lBWbN1zCwOSAeqwiu4+zKgFhjVwnM2Hveguxe5e1FOTs5RNKNzMjNuPm84Pzr/WF5cvJHL/vBPKmt3RzssEekCIpk45gFDzazAzBKAacCsJnVm8dkT6BcDs93dg2PiAIK1P4YTWg+kJeeUgJnx7dMH8/vLT2DZxhq+dP+7lJRvj3ZYItLJRSxxBIPb1wOvAMuAv7r7EjO7zcwuDKo9BGSZWQmhBwobb6+dQOhOqgXA08C17l55sHNGqg1dxZRR/Xhi+ins2lPPl+9/jzkryqMdkoh0YtYd7ropKiry4uLiaIcRdaVbdjL90fks31TDjWcP47pJhRo0F5GDMrP57l7UtDySl6qkgxmQ2YO//59TmTq6P7/9x0qmPzqfmjqt6SEiraPE0c0kJ8Ry56Vj+NkXR/DGinKm3vsuyzdp+XgRaTkljm7IzPj6aQU8/u2Tqd29jwvvfZdH3l+rhwVFpEWUOLqx8QWZvPT9iZw2JIufPLuEbz8yn6079kQ7LBHp4JQ4urns1ERmfP1EfnLBCN5aWcF5d7/NeyWV0Q5LRDowJQ7BzPjmhAL+fu2p9EiM5Wt/nMutzyxmx+59hz9YRLodJQ7Zb1RuOi98dyJXTyjgz3PX8S93vcV7q9X7EJEDKXHIAZITYrn1ghH89TunEBdjfO0P6n2IyIGUOKRZJ+Zn8tL3T+ebp4V6H2ff8SYvL96oO69ERIlDDi45IZaffHEET11zKunJ8Vzz5w/55sPzKN2yM9qhiUgUKXHIYY0b1IvnvzuBH3/hWOZ+soWz73iT++aUsGdfQ7RDE5EoUOKQFomLjeFbEwfz+k1nMHl4b/77lRVMufst5iwv1+UrkW5GiUNapV96Mr//13H86Rsn4g7feHgeV874gBWbNF27SHehxCFHZNIxvXnlhtO59YIRLCzdxnl3v8WPnl5ElRaLEunylDjkiCXExXD1hALe/MEkrjh5EE/MK+XM/36D+98oYdee+miHJyIRovU4pM2UlG/nVy8uZ/bycnLSEvne5EIuPXEgCXH6+0SkM9J6HBJxhb3TmPH1E/nbNadQkJXCrc8u4ew73uTpj8qob+j6f6CIdBdKHNLmTszP5MnvnMyfvnEiqYlx/N8nF3L+3W/zypJNNCiBiHR6ShwSEWbGpGN68/x3J/C7y8ayp76B7zw6n/PveZsXPt6oBCLSiWmMQ9rFvvoGnv94I7+bvYrVFTso7J3KdycXcsHx/YnVuuciHdLBxjiUOKRd1Tc4Ly4KJZCVm2spyE7hukmFTB3Tn/hYdYBFOhIlDiWODqWhwXl16Sbufr2EZRtr6J+exDcnFDBt/EBSE+OiHZ6IEKW7qsxsipmtMLMSM7u5mf2JZvZksH+umeUH5eeY2XwzWxT8OznsmDeCcy4IXr0j2QaJjJgYY8qofrz4vQk8dFUReZk9+MULyzjlP1/n9peXU15TF+0QReQgItbjMLNYYCVwDlAGzAMuc/elYXWuBY5392vMbBrwJXe/1MzGApvdfYOZjQJecffc4Jg3gH9z9xZ3IdTj6BwWlG7jwbdW8/LiTcTFxPClsbl8+/TBFPZOjXZoIt3SwXockbwmMB4ocfc1QQBPAFOBpWF1pgI/C94/BdxrZubuH4XVWQIkm1miu2s+iy5szIAM7r98HOuqdvDHtz/hr8WlPFlcyuThvfn6qflMKMwmRgPpIlEXyUtVuUBp2HZZUNZsHXffB1QDWU3qfAX4sEnS+FNwmepWM2v2m8TMpptZsZkVV1RUHE07pJ0NykrhPy4axXs3T+aGs4fycVk1V874gLPvfJOZ762lVqsRikRVh76NxcxGArcD3wkrvtzdjwMmBq8rmjvW3R909yJ3L8rJyYl8sNLmslITueHsYbx78yTuunQMaUnx/HTWEk7+1ev8bNYSPqncEe0QRbqlSF6qWg8MCNvOC8qaq1NmZnFAOlAFYGZ5wNPAle6+uvEAd18f/LvdzB4ndEnskUg1QqIvMS6Wi8bmctHYXD76dCsz31vLY3PX8fB7aznzmByuOjWfM4bm6DKWSDuJZOKYBww1swJCCWIa8LUmdWYBVwHvAxcDs93dzSwDeAG42d3fbawcJJcMd680s3jgAuC1CLZBOpixA3sxdmAv/v0Lx/L43E95bO6nfONP88jrlcxl4wdySVEevdOSoh2mSJcW0ec4zOx84C4gFpjh7r80s9uAYnefZWZJwKPAWGALMM3d15jZj4FbgFVhpzsX2AG8BcQH53wNuNHdDzmHt+6q6rr27Gvg5SWb+MvcT3l/TRVxMcbZx/bhspMGMlGD6SJHRQ8AKnF0eWsqanliXilPzS9jy4496oWIHCUlDiWObmP3vnpeXbKZx5v0Qr56Yh6nD80hTlObiLRINJ7jEImKxLhYvji6P18c3Z81FbU8Oa+Uv80v4+Ulm8hJS+RLY3O5eFwew/qkRTtUkU5JPQ7pFvbsa2DOinKeml/GnOXl7Gtwjs9L5+JxeVw4uj8ZPRKiHaJIh6NLVUocEqis3c2zCzbwt+JSlm/aTkJsDGeP6M0l4wYwcWi2LmWJBJQ4lDikGUs2VPPU/DKeXbCBLTv2kJ2ayBdH92PqmFxG56VzkIkJRLoFJQ4lDjmExktZf/+wjDnLK9hT30B+Vg8uHJPL1DH9GZKjiRal+1HiUOKQFqretZdXFm/i2YXreW91Fe4wKrcnU0fn8sXR/embrlt7pXtQ4lDikCOwuaaO5xZuYNbCDXxcVo0ZnFyQxdQx/Zkyqq8G1aVLU+JQ4pCjtKaillkLN/Dsgg18UrmDuBjj1MJsvnBcX84d0ZdeKUoi0rUocShxSBtxdxavr+GFRRt5cdFGPt2yk9gY49QhWXzhuH6cO7IvmUoi0gUocShxSAS4O0s2fJZE1lWFksgpg7M4/7h+/MvIPmSlJkY7TJEjosShxCER1phEXgySyNogiZw8OJMpI/tyzoi+GliXTkWJQ4lD2pG7s2zj9v1JZE2w6NTovHTOHdmXc0b0YWjvVD0nIh2aEocSh0RRSXktry7dxKtLNrOgdBsA+Vk9OHdkX84d0YexA3sRqyngpYNR4lDikA5ic00d/1i6mVeXbub91ZXsrXeyUhI4+9g+nDuyD6cVZpMUHxvtMEWUOJQ4pCOqqdvLmysqeHXpZuYsL6d29z56JMQyoTCbycN7M2l4b/r01LiIRIemVRfpgHomxe+fAn73vnr+uWYLry7ZxOzl5by6dDMAI/v33J9ERudl6JKWRJ16HCIdkLuzfNN25qwoZ87ycuav20qDQ2ZKAmcMy2HS8N6cMTSH9B7x0Q5VujBdqlLikE5s2849vLmygjnLy3lzZQVbd+4lNsYYN7AXk4b3ZvLw3gzro7u0pG0pcShxSBdR3+AsKN3KnOUVzF5eztKNNQD0S09i4tBsJg7NYUJhtqZAkaOmxKHEIV3Upuo65qwo5+1VFbyzqpKaun2YwXG56Zw+NIeJQ7MZO7AXCXFaoEpaJyqJw8ymAHcDscAf3f2/muxPBB4BxgFVwKXuvtbMzgH+C0gA9gA/cPfZwTHjgIeBZOBF4Pt+mEYocUh3Ud/gLCzbxtsrK3l7VQUflW6jvsFJSYjllCHZnD4s1CPJz+qhy1pyWO2eOMwsFlgJnAOUAfOAy9x9aVida4Hj3f0aM5sGfMndLzWzscBmd99gZqOAV9w9NzjmA+B7wFxCieMed3/pULEocUh3VVO3l/dKqnh7VQVvraqgdMsuAAZkJjNxaA6nDcnm5MGZmk9LmhWNxHEK8DN3/5dg+xYAd//PsDqvBHXeN7M4YBOQE96DsNCfRVVAPyATmOPuw4N9lwFnuvt3DhWLEodIyNrKHUESqeT91VXU7t4HwPC+aZw6JJtTh2QxfnAmPZN0t5ZE5zmOXKA0bLsMOOlgddx9n5lVA1lAZVidrwAfuvtuM8sNzhN+ztzmPtzMpgPTAQYOHHgUzRDpOvKzU8jPTuGKU/LZV9/Ax+ureX91Fe+truSxueuY8e4nxBgcl5fBaUOyOHVINuMG9SI5QU+yy2c69AOAZjYSuB04t7XHuvuDwIMQ6nG0cWginV5cbAwnDOzFCQN7cd2kQur21vPRp9t4f3Ul762u4sG31nD/G6tJiI1h7MCMUI+kMIvReRkaaO/mIpk41gMDwrbzgrLm6pQFl6rSCV2WwszygKeBK919dVj9vMOcU0SOQFJ8LKcMyeKUIVncCNTu3se8tVv290juen0ld74GSfExjB3Qi/EFmZxUkMnYgeqRdDeRTBzzgKFmVkDoy30a8LUmdWYBVwHvAxcDs93dzSwDeAG42d3fbazs7hvNrMbMTiY0OH4l8LsItkGk20pNjGPSMb2ZdExvIPQQ4j/XVDH3ky188MkW7pm9CneIjzWOy01nfEEWJxVkMi6/l8ZIurhI3457PnAXodtxZ7j7L83sNqDY3WeZWRLwKDAW2AJMc/c1ZvZj4BZgVdjpznX3cjMr4rPbcV8CvqvbcUXaX/WuvXy4bmuQSKpYtL6avfWOGYzo15PxBZmMz8/kxIJMsnXXVqekBwCVOEQiateeej4q3coHQY/kw0+3Ure3AYAhOSmML8jkhIG9GDeoFwXZKXqOpBNQ4lDiEGlXe/Y1sGh9dZBIqpi/bis1daHbf3v1iGfcoF6cMKgX4wb24vi8DI2TdEBKHEocIlHV0OCsrqhl/rqtodenW1lTEVpSNy7GGNm/ZyiRBK9+6clRjliUOJQ4RDqcLTv28NGnW/cnk4Vl2/Zf3srNSOaEQb04YWAGYwf24th+aSTGqVfSnrSQk4h0OJkpCZx1bB/OOrYPAHvrG1i2seazXsnaLTy3cAMQuntrRL+ejB6QwZgBGYwekEFBVgoxWtiq3anHISId2sbqXSws3caC0moWlG5lUVk1O/bUA5CWFMfovAxGD0hnzIBejB6QTu80LbXbVtTjEJFOqV96Mv3Sk5kyqh8QmgF4dUUtC0q3BQllG//z5hrqG0J/BPdPT2J00CMZMyCDUbnppCbqq64t6acpIp1KbIwxrE8aw/qk8dWi0OQUdXvrWbKhmgWl1fuTyUuLNwFgBgVZKYzKTWdUbk9G5aYzsn866cl6SPFIKXGISKeXFB/LuEGZjBuUub9sy449LCzdxuL11SxaX03x2i3MCsZLAAZl9WBU/3RG5aZzXG46I/v31KqJLaTEISJdUmZKApOG92bS8N77y6pqd7N4Qw2L11ezeH01H6/fxguLNu7fn9crmVH90zkuL5RIjstN11olzVDiEJFuIys1kTOG5XDGsJz9Zdt27mHJhhoWBclk8fpqXl6yaf/+Pj0TObZfz/2vEf3SKMhOJbYb382lxCEi3VpGjwROK8zmtMLs/WU1dXtZsj7UM1m2sYalG2t4Z1Ul+4IB+MS4GI7pm8aIsIQyvF9at5ncUbfjioi0wJ59DZSU17JsY83+ZLJsYw1bd+7dXyevV/IBPZNj+/VkQK8enfZZE92OKyJyFBLiYhjRvycj+vfcX+bubK7ZfUAiWbaxhteXbSbonJCSEMvQPmkM65PKsD5pHNM3jWP6pJGTlthpJ3pU4hAROUJmRt/0JPqmJx0wCL9rTz0rN29n2cYalm/azsrN25m9vJy/Fn+28nVGj3iG9U5jWN9UjgluLx7WJ61T3NmlxCEi0saSE2L3P4QYrrJ2Nys3b2flpu2sLK9l5abtPLtgA9uDWYMBctISwxJJKsP6pjG0dyppHWj8RIlDRKSdZKcmkp2ayKlDPhuId3c21dSxcnMokazYHOqhPP7Buv0TPgL07ZlEYe9UCnunMqR3KoU5offZqQntfslLiUNEJIrMbP+0KuG3CTc0OGVbd+1PJKvLaympqOVvxaX75+oCSE+OZ0hOyv6kUtg7lcKcNPJ6JUdsUF53VYmIdCLuzsbqOkrKa1ldUUtJee3+95W1e/bXS4yLYXBOKk9MP/mIp1fRXVUiIl2AmdE/I5n+GcmcHtZDgdDDjI2JpKS8ltKtO+mZ1PZf80ocIiJdREaPBIryMynKzzx85aMQE9Gzi4hIlxPRxGFmU8xshZmVmNnNzexPNLMng/1zzSw/KM8yszlmVmtm9zY55o3gnAuCV++m5xURkciJ2KUqM4sF7gPOAcqAeWY2y92XhlW7Gtjq7oVmNg24HbgUqANuBUYFr6Yud3eNdouIREEkexzjgRJ3X+Pue4AngKlN6kwFZgbvnwLOMjNz9x3u/g6hBCIiIh1IJBNHLlAatl0WlDVbx933AdVAVgvO/afgMtWtdpAnX8xsupkVm1lxRUVF66MXEZFmdcbB8cvd/ThgYvC6orlK7v6guxe5e1FOTk5zVURE5AhEMnGsBwaEbecFZc3WMbM4IB2oOtRJ3X198O924HFCl8RERKSdRDJxzAOGmlmBmSUA04BZTerMAq4K3l8MzPZDPMpuZnFmlh28jwcuABa3eeQiInJQEZ1yxMzOB+4CYoEZ7v5LM7sNKHb3WWaWBDwKjAW2ANPcfU1w7FqgJ5AAbAPOBdYBbwHxwTlfA25093oOwcwqgmOPRDZQeYTHdlZqc/egNncPR9PmQe7+uWv93WKuqqNhZsXNzdXSlanN3YPa3D1Eos2dcXBcRESiSIlDRERaRYnj8B6MdgBRoDZ3D2pz99DmbdYYh4iItIp6HCIi0ipKHCIi0ipKHAdxuCnhOzMzm2Fm5Wa2OKws08z+YWargn97BeVmZvcEP4ePzeyE6EV+ZMxsQDBN/1IzW2Jm3w/Ku3Kbk8zsAzNbGLT550F5QbCEQUmwpEFCUN7sEgedkZnFmtlHZvZ8sN2l22xma81sUTB/X3FQFtHfbSWOZoRNCX8eMAK4zMxGRDeqNvUwMKVJ2c3A6+4+FHg92IbQz2Bo8JoO/L6dYmxL+4Cb3H0EcDJwXfDfsyu3eTcw2d1HA2OAKWZ2MqGlC+5090JgK6GlDSBsiQPgzqBeZ/V9YFnYdndo8yR3HxP2vEZkf7fdXa8mL+AU4JWw7VuAW6IdVxu3MR9YHLa9AugXvO8HrAjePwBc1ly9zvoCniW0Tky3aDPQA/gQOInQE8RxQfn+33PgFeCU4H1cUM+iHfsRtDUv+KKcDDwPWDdo81ogu0lZRH+31eNoXkumhO9q+rj7xuD9JqBP8L5L/SyCyxFjgbl08TYHl2wWAOXAP4DVwDYPLWEAB7brSJc46GjuAv4f0BBsZ9H12+zAq2Y238ymB2UR/d2O2AqA0nm5u5tZl7tP28xSgf8FbnD3mvClXLpimz00h9sYM8sAngaGRzeiyDKzC4Byd59vZmdGOZz2NMHd11toGe1/mNny8J2R+N1Wj6N5LZkSvqvZbGb9AIJ/y4PyLvGzCGZT/l/gMXf/e1DcpdvcyN23AXMIXabJCJYwgAPb1eolDjqg04ALgwlSnyB0uepuunab8c+Wmign9AfCeCL8u63E0byWTAnf1YRPcX8VoXGAxvIrg7sxTgaqw7rAnYKFuhYPAcvc/Y6wXV25zTlBTwMzSyY0prOMUAK5OKjWtM0tXuKgI3L3W9w9z93zCf0/O9vdL6cLt9nMUswsrfE9oVnEFxPp3+1oD+x01BdwPrCS0HXhH0U7njZu21+AjcBeQtc4ryZ0bfd1YBWh6eozg7pG6A6z1cAioCja8R9BeycQug78MbAgeJ3fxdt8PPBR0ObFwE+C8sHAB0AJ8DcgMShPCrZLgv2Do92Go2z/mcDzXb3NQdsWBq8ljd9Vkf7d1pQjIiLSKrpUJSIiraLEISIiraLEISIiraLEISIiraLEISIiraLEIdKBmdmZjbO8inQUShwiItIqShwibcDM/jVY/2KBmT0QTDBYa2Z3ButhvG5mOUHdMWb2z2A9hKfD1kooNLPXgjU0PjSzIcHpU83sKTNbbmaPWfgkWyJRoMQhcpTM7FjgUuA0dx8D1AOXAylAsbuPBN4Efhoc8gjwQ3c/ntDTu43ljwH3eWgNjVMJPd0Podl8byC0NsxgQnMyiUSNZscVOXpnAeOAeUFnIJnQpHINwJNBnT8DfzezdCDD3d8MymcCfwvmG8p196cB3L0OIDjfB+5eFmwvILSWyjsRb5XIQShxiBw9A2a6+y0HFJrd2qTekc7vszvsfT36/1aiTJeqRI7e68DFwXoIjes9DyL0/1fjrKxfA95x92pgq5lNDMqvAN509+1AmZldFJwj0cx6tGcjRFpKf7mIHCV3X2pmPya0ClsMoVmHrwN2AOODfeWExkEgNM31/wSJYQ3wjaD8CuABM7stOMcl7dgMkRbT7LgiEWJmte6eGu04RNqaLlWJiEirqMchIiKtoh6HiIi0ihKHiIi0ihKHiIi0ihKHiIi0ihKHiIi0yv8HNnSKgw+JB38AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lr = 0.1  # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = [] # 将每轮的acc记录在此列表中，为后续画acc提供数据\n",
    "epoch = 500  # 循环500轮\n",
    "loss_all = 0 # 每轮分 4 个step,loss_all记录四个step生成 4 分loss的和\n",
    "\n",
    "# 嵌套循环迭代，with结构更新参数，显示当前loss\n",
    "for epoch in range(epoch): # 数据集级别的循环。每个epoch循环一次数据集\n",
    "    for step,(x_train,y_train) in enumerate(train_db):  # batch级别的循环，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train,w1) + b1\n",
    "            y = tf.nn.softmax(y)\n",
    "            y_ = tf.one_hot(y_train,depth=3)   # 将标签值转换为独热码格式\n",
    "            loss = tf.reduce_mean(tf.square(y_- y))\n",
    "            loss_all += loss.numpy()\n",
    "        # 计算loss对各参数的梯度\n",
    "        grads = tape.gradient(loss,[w1,b1])\n",
    "        \n",
    "        # 实现梯度更新\n",
    "        w1.assign_sub(lr*grads[0])\n",
    "        b1.assign_sub(lr*grads[1])\n",
    "    \n",
    "    # 每个epoch,打印loss信息\n",
    "    print(\"Epoch {}, loss:{}\".format(epoch,loss_all/4))\n",
    "    train_loss_results.append(loss_all/4)  #  将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0\n",
    "    \n",
    "    # 测试部分\n",
    "    # total_correct 为预测对的样本个数，total_number 为预测的总测试样本数，将这两个变量初始化为0\n",
    "    total_correct, total_number = 0,0\n",
    "    for x_test, y_test in test_db:\n",
    "        # 使用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test,w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y,axis=1)\n",
    "        pred = tf.cast(pred,dtype = y_test.dtype)\n",
    "        # 若分类正确，correct=1，否则为0 。将bool转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test),dtype=tf.int32)\n",
    "        # 将每个batch的correct数加起来\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        # 将所有batch中的correct数加起来\n",
    "        total_correct += int(correct)\n",
    "        # total_number为测试的样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "        \n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct/total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"test_acc:\",acc)\n",
    "    print(\"-------------------------\")\n",
    "    \n",
    "    \n",
    "plt.title(\"Loss Function Curve\")  # 标题\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss_results,label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出图像图标\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Acc Curve\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.plot(test_acc,label=\"$Accuracy$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th.where(条件语句，真返回A，假返回B)\n",
    "a = tf.constant([1,2,3,1,1])\n",
    "b = tf.constant([0,1,3,4,5])\n",
    "c = tf.where(tf.greater(a,b),a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5])>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 0.417022004702574\n",
      "b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01]\n",
      " [1.46755891e-01 9.23385948e-02 1.86260211e-01]]\n"
     ]
    }
   ],
   "source": [
    "# np.random.RandomState.rand()返回  [0,1)  之间的随机数\n",
    "# np.random.RandomState.rand(维度) 维度为空，返回标量\n",
    "\n",
    "import numpy as np\n",
    "rdm = np.random.RandomState(seed=1)\n",
    "a = rdm.rand()\n",
    "b = rdm.rand(2,3)\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# 纵方向叠加\n",
    "# np.vastack(a,b)\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = np.vstack((a,b))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[1. 1. 1. 1.]\n",
      " [2. 2. 2. 2.]]\n",
      "y: [[2.  2.5 3.  3.5]\n",
      " [2.  2.5 3.  3.5]]\n",
      "grid:\n",
      " [[1.  2. ]\n",
      " [1.  2.5]\n",
      " [1.  3. ]\n",
      " [1.  3.5]\n",
      " [2.  2. ]\n",
      " [2.  2.5]\n",
      " [2.  3. ]\n",
      " [2.  3.5]]\n",
      "[[[1.  1.  1.  1. ]\n",
      "  [2.  2.  2.  2. ]]\n",
      "\n",
      " [[2.  2.5 3.  3.5]\n",
      "  [2.  2.5 3.  3.5]]]\n"
     ]
    }
   ],
   "source": [
    "# np.mgrid[]       [起始值，结束值)\n",
    "# np.mgrid[起始值：结束值：步长，起始值：结束值：步长，...]\n",
    "# x.ravel()  # 将x变为一维数组\n",
    "# np.c_[]使返回的间隔数值点配对\n",
    "# np.c_[数组1，数组2，...]\n",
    "\n",
    "import numpy as np\n",
    "x,y = np.mgrid[1:3:1,2:4:0.5]\n",
    "grid = np.c_[x.ravel(),y.ravel()]\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "print('grid:\\n',grid)\n",
    "d = np.mgrid[1:3:1,2:4:0.5]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
