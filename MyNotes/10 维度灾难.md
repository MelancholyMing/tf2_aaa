#### 10 维度灾难

> 1. 事实证明，在高维空间中，许多事物的行为都迥然不同。例如，如果你在一个单位平面（$1 \times 1$的正方形）内随机选择一个点，那么这个点离边界的距离小于0.001的概率只有约0.4%（也就是说，一个随机的点不太可能刚好位于某个维度的 “极端” ）。但是，在一个10 000 维的单位超立方体（$1 \times 1\cdots \times1$立方体，一万个1）中，这个概率大于99.999999%。高维超立方体中大多数点都非常接近边界。
>
> 2. 可以明显的发现，当维度不断增大，内接球的体积会趋于 0 ，而超立方体的体积还是 1 。这种反直觉的发现解释了分类器维度灾难的相关问题：
>
>    ​		**在高维空间中，大部分训练数据都位于定义的特征空间立方体的拐角处**
>
> 3. K近邻的讨论是基于一个重要假设：任意测试样本 $x$ 附近任意小的 $\delta$ 距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为 “密采样”，然而，这个假设在现实任务中通常很难满足，例如若 $\delta = 0.001$ ，仅考虑单个属性，则仅需1000个样本点平均分布在归一化后的属性取值范围内，即可使得任意测试样本在其附近0.001距离范围内总能找到一个训练样本，此时最近邻分类器的错误率不超过贝叶斯最优分类器的两倍。然而，这仅是属性维数为 1 的情况，若有更多的属性，则情况会发生显著变化。例如假定属性维数为20，若要求样本满足密采样条件，则至少需 $(10^3)^{20} = 10^{60}$ 个样本。**显现实应用中属性维数经常成千上万，则要满足密采样条件所需的样本数目是无法达到的天文数字。**此外，许多学习方法都涉及距离计算，而高维空间会给距离计算带来很大的麻烦。
>
>    还有一个更麻烦的区别：如果你在单位平面中随机挑两个点，这两个点之间的平均距离大约为 0.52 。如果在三维空间的单位立方体中随机挑两个点，两点之间的平均距离大约为 0.66 。但是，如果在一个100万维的超立方体中随机挑两个点呢？平均距离大约为 408.25 （约等于 $\sqrt{1\,000\,000 /6}$）。这是非常违背直觉的，这个事实说明**高维数据集有很大可能是非常稀疏的：大多数训练实例可能彼此之间相距很远。**



###### 降维方法

1. 直接降维：
   + 直接删属性（比如Lasso）
   + 线性降维（PCA）
   + 非线性降维（KPCA`（先升维再降维）`、流形降维、lsomap、LLE）
   + 分类：投影（Projection）、流形学习（manifold learning）