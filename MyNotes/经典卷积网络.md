# 经典卷积网络

![cnn](D:\workspace\MyNotes\pic\cnn.jpg)



+ letnet:	Yann Lecun 1988提出，卷积网络开篇之作

![LeNet](pic\LeNet5.jpg)

<center>
    <b>图5-21 LeNet5网络结构</b>
</center>



+ AlexNet诞生于2012年， 是Hinton的代表作之一， 当年ImageNet竞赛的冠军， Top5错误率为16.4%

> AlexNet使用Relu激活函数，提升了训练速度，使用Dropout 缓解了过拟合

![AlexNet](pic\AlexNet.jpg)

<center>
    <b>图5-23 AlexNet网络结构</b>
</center>

![](pic\KerasAlexNet.jpg)

<center>
    <b>图5-24 Keras实现AlexNet模型</b>
</center>



+ VGGNet是2014年ImageNet竞赛的亚军，Top5错误率减少到了7.3%

>VGGNet使用小尺寸卷积核，在减少参数同时，提高了识别准确率。

![VGGNet16](pic\VGGNet16.jpg)

<center>
    <b>图5-25 VGGNet16网络结构图</b>
</center>



+ InceptionNet诞生于2014年，当年ImageNet竟赛冠军，Top5错误率为6.67%

  > InceptionNet引入了Inception结构快，在同一层网络内使用不用尺寸的卷积核，提升了模型感知力，使用了批标准化，缓解了梯度消失

一层内使用不同尺寸的卷积核，提升感知力（通过padding实现输出特征面积一致）；使用1 * 1卷积核，改变输出特征channel数（减少网络参数）。
InceptionNet即GoogLeNet，诞生于2015年，旨在通过增加网络的宽度来提升网络的能力，与VGGNet通过卷积层堆叠的方式（纵向）相比，是一个不同的方向（横向）。
显然，InceptionNet模型的构建与VGGNet及之前的网络会有所区别，不再是简单的纵向堆叠，要理解InceptionNet的结构，首先要理解它的基本单元，如图5-27所示。

![Inception](pic\InceptionNet.jpg)

<center>
    <b>图5-27 InceptionNet基本单元</b>
</center>

InceptionNet的一个显著特点是大量使用了1 * 1的卷积核，事实上，最原始的InceptionNet的结构是不包含1 * 1卷积的，如图5-29所示。

![inception_orign](pic\InceptionNet_orign.jpg)

<center>
    <b>图5-29 InceptionNet最原始的基本单元</b>
</center>



由图5-29可以更清楚地看出InceptionNet最初的设计思想，即通过不同尺寸卷积层和池化层的横向组合（卷积、池化后的尺寸相同，通道可以相加）来拓宽网络深度，可以增加网络对尺寸的适应性。但是这样也带来一个问题，所有的卷积核都会在上一层的输出上直接做卷积运算，会导致参数量和计算量过大（尤其是对于5 * 5的卷积核来说）。因此，InceptionNet在3 * 3、5 * 5的卷积运算前、最大池化后均加入了1 * 1的卷积层，形成了图5-24中的结构，这样可以降低特征的厚度，一定程度上避免参数量过大的问题。
那么1 * 1的卷积运算是如何降低特征厚度的呢？下面以5 * 5的卷积运算为例说明这个问题。假设网络上一层的输出为100 * 100 * 128（H *W * C），通过32 * 5 * 5（32个大小为5 * 5的卷积核）的卷积层（步长为1、全零填充）后，输出为100 * 100 * 32，卷积层的参数量为32 * 5 * 5 * 128 = 102400；如果先通过32 * 1 * 1的卷积层（输出为100 * 100 * 32），再通过32 * 5 * 5的卷积层，输出仍为100 * 100 * 32，但卷积层的参数量变为32 * 1 * 1 * 128 + 32 * 5 * 5 * 32 = 29696，仅为原参数量的30 %左右，这就是小卷积核的降维作用。
InceptionNet网络的主体就是由其基本单元构成的，其模型结构如图5-30所示。

![InceptionV1](pic\InceptioinNetV1.jpg)

<center>
    <b>图5-30 InceptionNet v1模型结构图</b>
</center>

InceptionNet网络不再像VGGNet一样有三层全连接层（全连接层的参数量占VGGNet总参数量的90 %），而是采用“全局平均池化+全连接层”的方式，这减少了大量的参数。
这里介绍一下全局平均池化，在tf.keras中用GlobalAveragePooling2D函数实现，相比于平均池化（在特征图上以窗口的形式滑动，取窗口内的平均值为采样值），全局平均池化不再以窗口滑动的形式取均值，而是直接针对特征图取平均值，即每个特征图输出一个值。通过这种方式，每个特征图都与分类概率直接联系起来，这替代了全连接层的功能，并且不产生额外的训练参数，减小了过拟合的可能，但需要注意的是，使用全局平均池化会导致网络收敛的速度变慢。
总体来看，InceptionNet采取了多尺寸卷积再聚合的方式拓宽网络结构，并通过1 * 1的卷积运算来减小参数量，取得了比较好的效果，与同年诞生的VGGNet相比，提供了卷积神经网络构建的另一种思路。但InceptionNet的问题是，当网络深度不断增加时，训练会十分困难，甚至无法收敛（这一点被ResNet很好地解决了）。



+ ResNet诞生于2015年，当年ImageNet竞赛冠军，Top5错误率为 3.57%

> ResNet提出了层间残差跳连，引入了前方信息，缓解梯度消失， 使神经网络层数增加成为可能。

![ResNet](pic\ResNet18.jpg)

<center>
    <b>图5-31 ResNet18网络结构图</b>
</center>

ResNet的核心是残差结构，如图5-32所示。在残差结构中，ResNet不再让下一层直接拟合我们想得到的底层映射，而是令其对一种残差映射进行拟合。若期望得到的底层映射为H(x)，我们令堆叠的非线性层拟合另一个映射F(x) := H(x) – x，则原有映射变为F(x) + x。对这种新的残差映射进行优化时，要比优化原有的非相关映射更为容易。不妨考虑极限情况，如果一个恒等映射是最优的，那么将残差向零逼近显然会比利用大量非线性层直接进行拟合更容易。
值得一提的是，这里的相加与InceptionNet中的相加是有本质区别的，Inception中的相加是沿深度方向叠加，像“千层蛋糕”一样，对层数进行叠加；ResNet中的相加则是特征图对应元素的数值相加，类似于python语法中基本的矩阵相加。

![ResNetConstruct](pic\ResNetConstruct.jpg)

<center>
    <b>图5-32 ResNet中的残差结构</b>
</center>

ResNet引入残差结构最主要的目的是解决网络层数不断加深时导致的梯度消失问题，从之前介绍的4种CNN经典网络结构我们也可以看出，网络层数的发展趋势是不断加深的。这是由于深度网络本身集成了低层/中层/高层特征和分类器，以多层首尾相连的方式存在，所以可以通过增加堆叠的层数（深度）来丰富特征的层次，以取得更好的效果。

![cnn5](pic\cnn_compare.jpg)

但如果只是简单地堆叠更多层数，就会导致梯度消失（爆炸）问题，它从根源上导致了函数无法收敛。然而，通过标准初始化（normalized initialization）以及中间标准化层（intermediate normalization layer），已经可以较好地解决这个问题了，这使得深度为数十层的网络在反向传播过程中，可以通过随机梯度下降（SGD）的方式开始收敛。

但是，当深度更深的网络也可以开始收敛时，网络退化的问题就显露了出来：随着网络深度的增加，准确率先是达到瓶颈（这是很常见的），然后便开始迅速下降。需要注意的是，这种退化并不是由过拟合引起的。对于一个深度比较合适的网络来说，继续增加层数反而会导致训练错误率的提升

ResNet解决的正是这个问题，其核心思路为：对一个准确率达到饱和的浅层网络，在它后面加几个恒等映射层（即y = x，输出等于输入），增加网络深度的同时不增加误差。这使得神经网络的层数可以超越之前的约束，提高准确率。图5-34展示了ResNet中残差结构的具体用法。

![](pic\resnetFrame.jpg)

<center>
    <b>图5-34 ResNet中的残差结构</b>
</center>

上图中的实线和虚线均表示恒等映射，实线表示通道相同，计算方式为H(x) = F(x) + x；虚线表示通道不同，计算方式为H(x) = F(x) + Wx，其中W为卷积操作，目的是调整x的维度（通道数）。

对于ResNet的残差单元来说，除了这里采用的两层结构外，还有一种三层结构，如图5-36所示。

![ResNetMeta](pic\resnetMeta.jpg)

<center>
    <b>图5-36 两层/三层残差单元</b>
</center>

两层残差单元多用于层数较少的网络，三层残差单元多用于层数较多的网络，以减少计算的参数量。
总体上看，ResNet取得的成果还是相当巨大的，它将网络深度提升到了152层，于2015年将ImageNet图像识别Top5错误率降至3.57 %。

![compare](pic\cnn5_1.jpg)

对上述的5种CNN经典结构进行总结，如图5-37所示。

![cnn5](pic\cnn5.jpg)