## 知识点：

1. 对于分类问题为什么一般采用交叉熵而不是欧氏距离？

   logistic 回归和 softmax 回归使用交叉熵而不用欧氏距离是因为前者的目标函数是**凸函数**，可以求得全局极小值点；用欧氏距离则无法保证。

   > 实验证明，理论推导是正确的：交叉熵使得梯度与绝对误差成正比，二范数导致梯度变得扭曲。参考资料
   > 总结：
   >
   > 1. 神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛，如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。
   > 2. 使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。
   > 3. 使用交叉熵损失是函数，误差越大参数的梯度也越大，能够快速收敛。



2. 交叉熵函数的Hessian矩阵正定,

   鞍点问题：Hessian矩阵不定，不是局部极值点

   

3. 机器学习面临挑战：
   梯度消失：Relu可缓解

   退化问题

   局部极值问题

   鞍点问题

4. OCR：光学字符识别

5. 特征工程的局限性：

   通用性差

   建模能力差

   维数灾难

6. 机器学习算法的瓶颈：

   建模能力差

   对复杂问题的泛化能力差

7. 典型的网络结构：

   自动编码器	AE

   受限玻尔兹曼机	RBM

   卷积神经网络	CNN

   循环神经网络	RNN

   生成对抗网络	GAN

8. 应用

   CV：

   人脸检测

   人脸识别

   行人检测

   图像分割

   边缘检测

   语音识别：

   循环神经网络

   LSTM

   CTC－连接主义时序分类

   自然语言处理（NLP）：

   循环神经网络

   seq2seq


   中文分词

   词性标注

   命名实体识别

   文本分类

   自动问答 ASR

   自动摘要

   机器翻译

$$
\delta
$$